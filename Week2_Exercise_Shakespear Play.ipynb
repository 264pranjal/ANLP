{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Shakespeare Play (Week 2) - Weighted TF-IDF\n",
    "- Construct a TF-IDF Matrix using log weighting for the corpus Shakespeare play.\n",
    "- Construct a query vector consisting of terms from the vocabulary and find the ranks of the plays with respect to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package shakespeare to C:\\Users\\yuvaraja\n",
      "[nltk_data]     manikandan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package shakespeare is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('shakespeare')\n",
    "from nltk.corpus import shakespeare\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random # for random choice of query vocab\n",
    "import math\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus Dictionary\n",
    "corpus_dict = {}\n",
    "for file_id in shakespeare.fileids():\n",
    "    corpus_dict[file_id] = {}\n",
    "\n",
    "# Get Tokens of each document\n",
    "#for file_id in shakespeare.fileids():\n",
    "#    doc = corpus_dict[file_id]\n",
    "#    doc['words'] = shakespeare.words(file_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(terms_list):\n",
    "    '''\n",
    "    Input:\n",
    "        Document Terms as Iterable\n",
    "    Output:\n",
    "        Returns list of Terms after case folding\n",
    "    '''\n",
    "    return [w.lower() for w in terms_list]\n",
    "\n",
    "def removeAlphaNumeric(terms_list):\n",
    "    '''\n",
    "    Input:\n",
    "        Document Terms as Iterable\n",
    "    Output:\n",
    "        Returns list of terms having only alphabets\n",
    "    '''\n",
    "    return [w for w in terms_list if w.isalpha()]\n",
    "\n",
    "def removeStopWords(terms_list):\n",
    "    '''\n",
    "    Input:\n",
    "        Document Terms as Iterable\n",
    "    Output:\n",
    "        Returns list of Terms having no english stop words\n",
    "    '''\n",
    "    return [w for w in terms_list if w not in stop_words]\n",
    "\n",
    "def doStemming(terms_list):\n",
    "    '''\n",
    "    Input:\n",
    "        Document Terms as Iterable\n",
    "    Outptu:\n",
    "        Returns list after performing stemming over given terms\n",
    "    '''\n",
    "    return [ps.stem(w) for w in terms_list]\n",
    "\n",
    "def doLemmatization(terms_list):\n",
    "    '''\n",
    "    Input:\n",
    "        Document Terms as Iterable\n",
    "    Output:\n",
    "        Returns list after performing lemmatization over given terms\n",
    "    '''\n",
    "    return [lemmatizer.lemmatize(w) for w in terms_list]\n",
    "\n",
    "def preProcessWords(terms_list):\n",
    "    '''\n",
    "    Input:\n",
    "        Document Terms as Iterable\n",
    "    Outptu:\n",
    "        Returns Terms as List, in which\n",
    "            Terms are Normalized (case folding)\n",
    "            Removes AlphaNumeric Terms\n",
    "            Removes English Stop Words\n",
    "            Performs Stemming over the Terms\n",
    "    '''\n",
    "    terms_list = normalize_text(terms_list)\n",
    "    terms_list = removeAlphaNumeric(terms_list)\n",
    "    terms_list = removeStopWords(terms_list)\n",
    "    terms_list = doStemming(terms_list)\n",
    "    terms_list = doLemmatization(terms_list)\n",
    "    return terms_list\n",
    "    \n",
    "# preprocess the text\n",
    "for file_id in shakespeare.fileids():\n",
    "    doc = corpus_dict[file_id]\n",
    "    play_terms = shakespeare.words(file_id)\n",
    "    doc['words'] = preProcessWords(play_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary set for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getVocabulary(doc_terms_list):\n",
    "    '''\n",
    "    Input:\n",
    "        doc_terms: Document Terms as Iterable\n",
    "    Output:\n",
    "        return a set of given document terms\n",
    "    '''\n",
    "    return set(doc_terms_list)\n",
    "\n",
    "for file_id in shakespeare.fileids():\n",
    "    doc = corpus_dict[file_id]\n",
    "    doc['vocab'] = getVocabulary(doc['words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Word Frequency List for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCounts(doc_terms_list):\n",
    "    '''\n",
    "    Input:\n",
    "        Document Terms as Iterable\n",
    "    Output:\n",
    "        Returns a Dictionary, having\n",
    "            Each Term as a Key\n",
    "            Count of each Term as its value\n",
    "    '''\n",
    "    counts = {}\n",
    "    for w in doc_terms_list:\n",
    "        count = counts.get(w, 0)\n",
    "        counts[w] = count + 1\n",
    "    return counts\n",
    "    \n",
    "for file_id in shakespeare.fileids():\n",
    "    doc = corpus_dict[file_id]\n",
    "    doc['counts'] = getWordCounts(doc['words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Term Frequency List for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTermFrequency(doc_vocab_list, doc_count_dict, T):\n",
    "    '''\n",
    "    Input:\n",
    "        doc_vocab_list: Document Vocabulary as Iterable\n",
    "        doc_count_dict: Document Word Frequency as Dictionary\n",
    "        T: Total Number of Words in the Document\n",
    "    Output:\n",
    "        Returns a Dictionary, having\n",
    "            Each Term as a Key\n",
    "            Term Frequency of each Term as its value\n",
    "            TF = C/T\n",
    "                C: Count of the Term\n",
    "                T: Total Words In Document\n",
    "    '''\n",
    "    tf = {}\n",
    "    for w in doc_vocab_list:\n",
    "        tf[w] = doc_count_dict[w] / T\n",
    "    return tf;\n",
    "\n",
    "for file_id in shakespeare.fileids():\n",
    "    doc = corpus_dict[file_id]\n",
    "    doc['tf'] = getTermFrequency(doc['vocab'],doc['counts'], len(doc['words']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Corpus Vocabulary List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTotalVocabList(corpus_dict):\n",
    "    '''\n",
    "    Input:\n",
    "    from the given corpus dict,\n",
    "    construct total vocab set and returns it\n",
    "    '''\n",
    "    total_corpus_vocab = set() # empty set\n",
    "    for k,v in corpus_dict.items():\n",
    "        total_corpus_vocab = total_corpus_vocab.union(corpus_dict[k]['vocab'])\n",
    "    return total_corpus_vocab\n",
    "\n",
    "# Build list of Vocabulary list\n",
    "vocab_list = []\n",
    "for file_id in shakespeare.fileids():\n",
    "    doc = corpus_dict[file_id]\n",
    "    vocab_list.append(doc['vocab'])\n",
    "\n",
    "from itertools import chain\n",
    "corpus_total_vocab = set(chain.from_iterable(vocab_list))\n",
    "#pprint(total_corpus_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Document Frequency List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocumentFrequency(corpus_dict, total_corpus_vocab):\n",
    "    '''\n",
    "    from the given docs dicts,\n",
    "    construct Document Frequency list\n",
    "    and returns it\n",
    "    '''\n",
    "    df = {}\n",
    "    for w in total_corpus_vocab:\n",
    "        df_count = 0\n",
    "        for file_id,doc in corpus_dict.items():\n",
    "            if w in corpus_dict[file_id]['vocab']:\n",
    "                df_count = df_count + 1\n",
    "        df[w] = df_count\n",
    "    return df\n",
    "            \n",
    "corpus_df = getDocumentFrequency(corpus_dict, corpus_total_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInverseDocumentFrequency(corpus_df, total_corpus_vocab):\n",
    "    '''\n",
    "    using provided document frequency\n",
    "    constructs IDF for each vocab in total_vocab\n",
    "    and returns it as dict\n",
    "    '''\n",
    "    idf = {}\n",
    "    for w in total_corpus_vocab:\n",
    "        idf[w] = corpus_df[w] / len(shakespeare.fileids())\n",
    "    return idf\n",
    "\n",
    "corpus_idf = getInverseDocumentFrequency(corpus_df, corpus_total_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Weighted TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeightedTFIDFOfATerm(tc, idf):\n",
    "    '''\n",
    "    Input:\n",
    "        tc: Term Count of a Term\n",
    "        idf: Inverse Document Frequency of a Term\n",
    "    Output:\n",
    "        Computes Weighted TF-IDF and returns it\n",
    "    '''\n",
    "    wv = 0\n",
    "    if(tc > 0):\n",
    "        wv = 1 + np.log10(tc)\n",
    "    #print('tc: ', tc, 'idf: ', idf, 'wf-idf: ', wv * idf)\n",
    "    return wv * idf\n",
    "\n",
    "def getWeightedTFIDFOfADocument(doc_vocab, doc_count, idf):\n",
    "    '''\n",
    "    Input:\n",
    "        doc_vocab: Document Vocabulary as Iterable\n",
    "        doc_count: Document Word Frequency as Dictionary\n",
    "        idf: Inverse Document Frequency of the Corpus as Dictionary\n",
    "    Output:\n",
    "        Dictionary having Wv-TF-IDF for each term in doc_vocab\n",
    "    '''\n",
    "    wv_tf_idf = {}\n",
    "    for w in doc_vocab:\n",
    "        wv_tf_idf[w] = getWeightedTFIDFOfATerm(doc_count[w], idf[w])\n",
    "    return wv_tf_idf\n",
    "\n",
    "for file_id,doc in corpus_dict.items():\n",
    "    vocab = corpus_dict[file_id]['vocab']\n",
    "    counts = corpus_dict[file_id]['counts']\n",
    "    doc = corpus_dict[file_id]\n",
    "    doc['wv_tf_idf'] = getWeightedTFIDFOfADocument(vocab, counts, corpus_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Weighted TF-IDF DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_headers = shakespeare.fileids()\n",
    "tfidf_matrix = pd.DataFrame(columns=col_headers)\n",
    "\n",
    "for w in corpus_total_vocab:\n",
    "    wv = []\n",
    "    for file_id,doc in corpus_dict.items():\n",
    "        wv.append(doc['wv_tf_idf'].get(w, 0))\n",
    "    tfidf_matrix.loc[w] = wv\n",
    "\n",
    "#for file_id,doc in docs.items():\n",
    "#    for w in doc['vocab']:\n",
    "#        df.loc[w][file_id] = doc['wv_tf_idf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort based on vocabulary\n",
    "tfidf_matrix.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Query Vocabulary\n",
    "- Randomly selecting query words from Corpus Vocabulary\n",
    "- Out-of-Vocabulary (OOV) is not considered in this test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_query_words = len(total_corpus_vocab) # number of vocab words that we need to select\n",
    "# max_query_words = int(len(corpus_total_vocab)/8)\n",
    "max_query_words = 20\n",
    "print('max_query_words: ', max_query_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_words = random.choices(list(corpus_total_vocab), k=max_query_words)\n",
    "print('query_words: ', query_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build  Query Document\n",
    "query_doc = {}\n",
    "query_doc['words'] = preProcessWords(query_words)\n",
    "query_doc['vocab'] = getVocabulary(query_doc['words'])\n",
    "query_doc['counts'] = getWordCounts(query_doc['words'])\n",
    "query_doc['tf'] = getTermFrequency(query_doc['vocab'], query_doc['counts'], len(query_doc['words']))\n",
    "query_doc['wv_tf_idf'] = getWeightedTFIDFOfADocument(query_doc['vocab'], query_doc['counts'], corpus_idf)\n",
    "\n",
    "# for terms that don't exist in query words, assign 0 as Weighted TF-IDF for those terms\n",
    "for w in corpus_total_vocab:\n",
    "    count = query_doc['wv_tf_idf'].get(w, 0)\n",
    "    query_doc['wv_tf_idf'][w] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(a, b):\n",
    "    return 1 - cosine_similarity(a,b)\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tf_df_matrix = pd.DataFrame(columns=['Wv-TF-IDF'])\n",
    "\n",
    "for file_id,doc in corpus_dict.items():\n",
    "    cos_sim= []\n",
    "    doc_val = np.array(tfidf_matrix[file_id]).T\n",
    "    pprint(doc_val.shape)\n",
    "    query_val = np.array(list(query_doc['wv_tf_idf'].values()))\n",
    "    query_tf_df_matrix.loc[file_id] = cosine_similarity(doc_val, query_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tf_df_matrix.sort_values(by='Wv-TF-IDF',ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tf_df_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Query Document with Out-Of-Vocabulary (OOV)\n",
    "- TO-DO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
