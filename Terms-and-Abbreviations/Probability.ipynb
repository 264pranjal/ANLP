{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "## [Definition](#definition)\n",
    "## [Probability in NLP](#probability_in_nlp)\n",
    "### [Why Probability used in NLP ](why_probability_used_in_nlp)\n",
    "### [How Probability used in NLP](#how_probability_used_in_nlp)\n",
    "### [Discrete Sample Space](#discrete_sample_space)\n",
    "### [Probability Mass Function](#probability_mass_function)\n",
    "### [Sample Space Constraints](#sample_space_constraints)\n",
    "### [Events](#events)\n",
    "### [Random Variable](#random_variable)\n",
    "### [Probability of the sentence](#probability_of_a_sentence)\n",
    "### [Chain Rule](#chain_rule)\n",
    "### [Markove Assumption](#markov_assumption)\n",
    "### [Language Modeling](#language_modeling)\n",
    "#### [Langugae Model Parameters](#language_model_parameters)\n",
    "#### [Choosing n-gram for Language Model](#choosing_ngram_for_lm)\n",
    "#### [Reducing number of parameters](#reducing_no_of_parameters)\n",
    "### [Prior Probability](#prior_probability)\n",
    "### [Conditional Probaility (Posterior Probability)](#conditional_probability)\n",
    "### [Joint Probability](#joint_probability)\n",
    "## [Perplexity](#perplexity)\n",
    "### [Why Perplexity](#why_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition <a class='anchor' name='definition'></a>\n",
    "\n",
    "Probability is defined as the likelihood that an event will occur\n",
    "\n",
    "Eg., Flipping a coin. There is a 50% chance or probability that heads will come up for any given toss of a fair coin.\n",
    "\n",
    "Probability can be expressed as\n",
    "\n",
    "- as a Percentage - Eg., 60%\n",
    "- as a Decimal Form - Eg., 0.6\n",
    "- as a Fraction - 6/10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability in NLP <a class='anchor' name='probability_in_nlp'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Probability used in NLP <a class='anchor' name='why_probability-used_in_nlp'></a>\n",
    "- Probability will be used in estimating what could be the next word in the sentence\n",
    "- Provides methods to predict or make decisions to pick the next word in the sequence based on sampled data\n",
    "- Make the informed decision when there is a certain degree of uncertainty and some observed data\n",
    "    - Example: How * you?\n",
    "    - Finding all the possible words that might appeat in between How and you\n",
    "    - To get an understanding, see Google NGram Viewer\n",
    "- It provides a quantitative description of the chances or likelihood's associated with various outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Probability used in NLP <a class='anchor' name='how_probability_used_in_nlp'></a>\n",
    "\n",
    "1. Probability of a Sentence\n",
    "    - Which sentence is most likely (probable)\n",
    "2. Probability of the next word in the sentence?\n",
    "    - How likely to predict \"you\" as the next word after the query sentence \"How are ____?\"\n",
    "        - Likelihood of the next word is formalized through an observation by conducting experiment - counting the words in a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Sample Space <a class='anchor' name='discrete_sample_space'></a>\n",
    "- Consider the following Bag of Words (_count = 52_)\n",
    "    - Experiment\n",
    "        - Extracting tokens from a document\n",
    "    - Outcome\n",
    "        - Every token/word in _x_ in the document\n",
    "    - Sample Document\n",
    "        - A weather balloon is floating at a constant height above Earth when it releases a pack of instruments. (Level 1) a. If the pack hits the ground with a downward velocity of −73.5 m/s, how far did the pack fall? b. Calculate the distance the ball has rolled at the end of 2.2 s \n",
    "- The outcome of the experiment - 52 sample (words).\n",
    "    - They constitute the _sample space_, $\\Omega$ or the set of all possible outcomes\n",
    "        - $\\Omega$ = 'a', 'weather', 'balloon', 'is', 'floating', 'at', 'a', 'constant', 'height', 'above', 'earth', 'when', 'it', 'releases', 'a', 'if', 'the', 'pack', 'hits', 'the', 'ground', 'with', 'a', 'downard', 'velocity', 'of', 'm', 's', 'how', 'far', 'did', 'the', 'pack', 'fall', 'b', 'calculate', 'the', 'distance', 'the', 'ball', 'has', 'rolled', 'at', 'the', 'end', 'of', 's'\n",
    "- Each word in this sample belongs to $\\Omega$, represented by $x \\in \\Omega$\n",
    "- Eacm sample $x \\in \\Omega$ is assigned a probability score $[ 0, 1 ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Mass Function <a class='anchor' name='probability_mass_function'></a>\n",
    "- Probability Function | Probability Distribution Function\n",
    "    - A _probability function_ or _probability distribution function_ distributes the probability mass of $1$ to the all the samples in the sample space $\\Omega$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Space Constraints <a class='anchor' name='sample_space_constraints'></a>\n",
    "- All the words in the $\\Omega$, must satisfy the following constraints:\n",
    "    1. $P(x) \\in [0,1], for all x \\in \\Omega$\n",
    "    2. $\\sum_{x \\in \\Omega} P(x) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Events <a class='anchor' name='events'></a>\n",
    "\n",
    "- Events can be described as a variable taking a certain value\n",
    "- An __*Event*__ is a collection of samples of the same type, $E \\subseteq \\Omega$\n",
    "    - $P(E) = \\sum_{x \\in E} P(x)$\n",
    "- Example\n",
    "    - Consider above sample document\n",
    "        - Total number of words = 52.\n",
    "        - The number of __*unique*__ words = 37 or there are 37 __*types*__ of words in this BOW.\n",
    "        - 15 words have frequencies $> 1$.\n",
    "        - Example 1:\n",
    "            - $E_{pack} = 3$\n",
    "                - In above corpus, the word type $pack$ occurs $3$ times\n",
    "            - $P(E='pack')=\\frac{3}{52}=0.058$\n",
    "        - Example 2:\n",
    "            - $E_{the} = 6$\n",
    "                - In above corpus, the word type $the$ occurs $6$ times\n",
    "            - $P(E='the')=\\frac{6}{52}=0.115$\n",
    "        - Example 3:\n",
    "            - $E_{weather} = 1$\n",
    "                - In above corpus, the word type $weather$ occurs only $1$ time\n",
    "            - $P(E='weather')=\\frac{1}{52}=0.019$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Variable <a class='anchor' name='random_variable'></a>\n",
    "\n",
    "- A __random variable__[8], is a variable whose possible values are numerical outcomes of a random experiment\n",
    "- Two types of random variable\n",
    "    - Continuous\n",
    "    - Discrete\n",
    "- For NLP, it will be __*Discrete*__\n",
    "\n",
    "- To capture Type-Token distinction, we use random variable $W$.\n",
    "    - $W(x)$ maps to the sample $x \\in \\Omega$\n",
    "- $V$ is the set of types and the value is represented by a variable $v$\n",
    "- Given a random variable $V$ and a value $v$, $P(V = v)$ is the probability of the event that $V$ takes the value $v$, i.e: $P(V = v) = P(x \\in \\Omega: V(x) = v)$\n",
    "    - Example: $P(V = 'the') = P('the') = 0.115$\n",
    "- Random variables are useful in describing/ constructing various events\n",
    "- In NLP, we will often consider random variables representing the experiment of choosing a word within a vocabulary or choosing a sentence within a language. [11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of the sentence $W$ <a class='anchor' name='probability_of_a_sentence'></a>\n",
    "\n",
    "> $P(W) = P(w_1, w_2, ..., w_n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule <a class='anchor' name='chain_rule'></a>\n",
    "\n",
    "> $P(w_1, w_2, ..., w_n) = P(w_1)P(w_2|x_1)...P(w_n|w_1,...w_{n-1})$\n",
    "\n",
    "> $P('I got this one') = P('I', 'got', 'this', 'one'')$\n",
    "\n",
    "> $P('I got this one') = P('I') × P('got' | 'I') × P('this' | 'I got') × P('one' | 'I got this')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markove Assumption <a class='anchor' name='markov_assumption'></a>\n",
    "- From [12]\n",
    "    - The porbability of a word depends only on the $k-1$ preivous words (history).\n",
    "        - $P(w_n|w_1,w_2,...,w_{n-1})=P(w_n|w_{n+1-k}...w_{n-1})$\n",
    "        - Example: $k=2$\n",
    "            - $P('I got this one') = P('I') × P('got' | 'I') × P('this' | 'got') × P('one' | 'this')$\n",
    "    - This is called __Markov Assumption__: only the closes $k$ words are relvant:\n",
    "        - *Unigram*: previous words do not matter\n",
    "        - *Bigram*: only the previous word matters (like above example)\n",
    "        - *Trigram*: only the previous two words matter\n",
    "            - $P(nextWord | prevWord2 prevWord1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Modeling <a class='anchor' name='language_modeling'></a>\n",
    "\n",
    "- From [9]\n",
    "    - Computing the probability of a Sentence $P(W)$\n",
    "        - That is a model which computes $P(W)$ is the __language model__\n",
    "            - A better term for this would be \"The Grammar\"\n",
    "            - But \"Language Model\" or LM is standard\n",
    "    - Which sentence is most likely (most probable)?\n",
    "        - I saw this dog running across the street.\n",
    "        - Saw dog this I running across street the.\n",
    "    - Why? You have a *language model* in your head\n",
    "        - $P(\"I saw this\") >> P(\"saw dog this)$\n",
    "- From [12]\n",
    "    - A language model is a probability distribution over word/character sequences\n",
    "    - We would like to find a language model $P$, such that\n",
    "        - $P(\"And nothing but the truth\") \\approx 0.001$\n",
    "        - $P(\"And nuts sing on the roof\") \\approx 0.000000001$\n",
    "    - **__Bigram Model__**\n",
    "        - $P(but | nothing) = \\frac{P(nothing \\; but)}{P(nothing)} \\approx \\frac{C_1}{C_2}$\n",
    "            - Let $C_1$ be the count of how many times the phrase \"nothing but\" occured in the training corpus\n",
    "            - Let $C_2$ be the count of how many times the token \"nothing\" occured in the training corpus\n",
    "    - **__Trigram Model__**\n",
    "        - $P(the | nothing but) = \\frac{P(nothing but the)}{P(nothing but)} \\approx \\frac{C_1}{C_2}$\n",
    "            - Let $C_1$ be the count of how many times the phrase \"nothing but the\" occured in the training corpus\n",
    "            - Let $C_2$ be the count of how many times the phrase \"nothing but\" occured in the training corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langugae Model Parameters <a class='anchor' name='anguage_model_parameters'></a>\n",
    "\n",
    "- From [12]\n",
    "    - Each Probability factor in probability of a sentence is called as __model parameters__\n",
    "    - Example: Markov Assumption (bigram model)\n",
    "        - $P('I got this one') = P('I') × P('got' | 'I') × P('this' | 'got') × P('one' | 'this')$\n",
    "            - Each probabilty in above equation is a model parameter\n",
    "    - The number of n-grams is exactly the number of parameters we have to learn\n",
    "        - In above example, since we have selected bigram, it had 4 model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choosing n-gram for Language Model <a class='anchor' name='choosing_ngram_for_lm'></a>\n",
    "\n",
    "- From [12]\n",
    "    - Largen n\n",
    "        - __greater discrimnation__:\n",
    "            - more information about the context of the specific instance\n",
    "        - but __less reliability__:\n",
    "            - Out model is too complex, that it has too many parameters\n",
    "            - Cannot estimate parameters reliably from limited data (data sparseness)\n",
    "                - too many chances that the history has never been seen before\n",
    "                - our estimates are not reliable because we have not seen enough examples\n",
    "    - Small n\n",
    "        - __less discrimination__:\n",
    "            - not enough history to predict the next word very well, or model is not so good\n",
    "        - but __more reliable__:\n",
    "            - more instances in training data, better statistical estimates of our parameters\n",
    "    - Bigrams and Trigrams are used in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducing number of parameters <a class='anchor' name='reducing_no_of_parameters'></a>\n",
    "\n",
    "- From [12]\n",
    "    - To reduce the number of parameters, we can:\n",
    "        - do stemming (use stems instead of word types)\n",
    "            - help = helps = helped\n",
    "        - group words into semantic classes\n",
    "            - {Monday, Tuesday, Wednesday, Thursday, Friday} = one word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Probability <a class='anchor' name='prior_probability'></a>\n",
    "\n",
    "- From [10]\n",
    "    - *Prior Probability*: The probability before we consider any additional knowledge\n",
    "        - $P(A)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Probaility (Posterior Probability) <a class='anchor' name='conditional_probability'></a>\n",
    "\n",
    "- From [7]\n",
    "    - The conditional probability $P(E_2 | E_1)$ is the probability of event $E_2$ given that even $E_1$ hs occured.\n",
    "        - You can think of this as the probability of $E_2$ givent hat $E_1$ is the temporary sample set\n",
    "    - $P(E_2 | E_1) = \\frac{P(E_1,E_2)} {P(E_1)}$ if $P(E_1) > 0$\n",
    "- From [10] \n",
    "- From [12]\n",
    "    - $P(X|Y)$ means probabilty that X is true when we already know Y is true\n",
    "        - P(baby is named John | baby is a boy) = 0.002\n",
    "        - P(baby is a boy | baby is named John) = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Probability <a class='anchor' name='joint_probability'></a>\n",
    "\n",
    "- From [12]\n",
    "    - $P(X,Y)$ means that X and Y are both true, for example:\n",
    "        - P(brown eyes, boy) = (number of all baby boys with brown eyes)/(total number of babies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity <a class='anchor' name='perplexity'></a>\n",
    "\n",
    "- From [9]\n",
    "    - Perplexity is the probability of the test set (assigned by the language model) normalized by the number of words.\n",
    "        - $PP(W) = P(w_1,w_2,...w_N)$<sup>$-\\frac{1}{N}$</sup>\n",
    "        - $PP(W) = \\sqrt[N]{\\frac{1}{{P(w_1,w_2,\\ldots, w_N)}}}$\n",
    "        - Chain Rule: $PP(W) = \\sqrt[N]{\\prod_{i=1}^{N}\\frac{1}{P(w_i|w_1,...,w_{i-1})}}$\n",
    "        - For bigrams: $PP(W) = \\sqrt[N]{\\prod_{i=1}^{N}\\frac{1}{P(w_i|w_{i-1})}}$\n",
    "        \n",
    "    - Minimizing perplexity is the same as maximizing probability\n",
    "        - The best language model is one that best predicts an unseen test set\n",
    "        - __Lower Perplexity = Better Model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Perplexity <a class='anchor' name='why_perplexity'></a>\n",
    "- From [9]\n",
    "    - Say we have learned probabilities from a __training set__.\n",
    "    - Next we need to look at the model's performance on some new data\n",
    "        - This is a __test set__. A dataset different tha our training set\n",
    "    - Then we need an __evaluation metric__ to tell us how well our model is doing on the test set.\n",
    "    - One such metric is __perplexity__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
