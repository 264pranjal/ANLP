{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Word Learning Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [v1] Lecture 44\n",
    "  - Finding the bi-gram\n",
    "    - Given a word, we want network to learn the next word\n",
    "  - Simple architecture to understand the Word Embedding concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Preparation for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets take window size $n=5$ and assume the given corpus is preprocessed already (removing letters, symbols, numbers, ...)\n",
    "- Assume we have start $<S>$ and end $</S>$ symbols present in the corpus\n",
    "- Start from first word (assuming two start symbols are added), start building $bi-grams$\n",
    "- Repeatitions in the bigram model doesn't matter\n",
    "\n",
    "![Source_Preparation_For_Training](images/Source_Preparation_For_Training.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Word Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let $V$ be the size of the input (input vector size)\n",
    "  - $x_i = (x_1, x_2, \\ldots , x_V$)\n",
    "  - Example: If we have 1 million words in the vocabulary, then $||V|| = \\text{1 million}$, i.e., the number of elements in the input layer is 1 million\n",
    "  - Input will be __*BoW - Bag of Words*__\n",
    "- The hidden layer size will less than the input layer size\n",
    "  - Usually it may go upto $300$, or $500$\n",
    "- The size of output layer will be same as Input Layer\n",
    "  - So if we have 1 million elements in Input layer, the size of output layer will also be 1 million\n",
    "\n",
    "![One_Word_Learning](images/One_Word_Learning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input will be __*BoW - Bag of Words*__ and the representation will be __*OHV - One Hot Vector*__\n",
    "\n",
    "![One_Word_Learning_Input_Layer](images/One_Word_Learning_Input_Layer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is a fully connected layer\n",
    "\n",
    "![One_Word_Learning_Hidden_Layer](images/One_Word_Learning_Hidden_Layer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![One_Word_Learning_Output_Layer](images/One_Word_Learning_Output_Layer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Weights - Hidden Output Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \n",
    "\n",
    "![Update_Weights__Hidden_Output_Layers](images/Update_Weights__Hidden_Output_Layers.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why Cross Entropy?\n",
    "  - $log$ $p(x)$ is well scaled\n",
    "  - Selection of step size is easier\n",
    "  - With $p(x)$ multiplication may yield to near zero causing _underflow_\n",
    "  - For better optimization, $log$ $p(x)$ is considered (multiplication $\\rightarrow$ addition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is Cross Entopy? From <https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation>\n",
    "  - The Cross Entrophy formula takes in two distributions, $p(x)$, the true distribution, and $q(x)$, the estimated distribution, defined over the discreate variable $x$ and is given by\n",
    "    - $\\Large H(p,q) = - \\Large \\Sigma p(x) log(q(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From <https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html>\n",
    "\n",
    "```python\n",
    "def CrossEntropy(yHat, y):\n",
    "    if y == 1:\n",
    "      return -log(yHat)\n",
    "    else:\n",
    "      return -log(1 - yHat)\n",
    "```\n",
    "\n",
    "![Cross_Entropy_Log_Loss_Function_Graph](images/Cross_Entropy_Log_Loss_Function_Graph.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Weights (HO) - Minimization of E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Update_Weights_(HO)_Minimization_Of_E](images/Update_Weights_(HO)_Minimization_Of_E.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Input to Hidden Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Update_Input_to_Hidden_Weights](images/Update_Input_to_Hidden_Weights.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Insights on Output-Hidden-Input Layer Weight Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Some_Insights_on_Output_Hidden_Input_Layer_Weight_Updates](images/Some_Insights_on_Output_Hidden_Input_Layer_Weight_Updates.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Softmax Calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See [v1] Lecture 42\n",
    "\n",
    "![Softmax_Calculation](images/Softmax_Calculation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How differences calcualted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See [v1] Lecture 43\n",
    "\n",
    "![How_Differences_Calculated](images/How_Differences_Calculated.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
