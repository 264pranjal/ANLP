{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "- From NLP Perspective\n",
    "    - Another mechanism to process the corpus and get insights out of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why we need Machine Learning (Neural Network)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In order to understand the need of NN, we need to understand the limitations of standard algorithms\n",
    "    - __*Standard Algorithm*__: An algorithm is a sequence of instructions to solve a problem\n",
    "        - The steps to solve problems are well defined\n",
    "            - We know _what are the inputs_\n",
    "            - We _know what are the rules to manipulate the input_, and\n",
    "            - We _know what we expect out of program as output_\n",
    "        - Steps are coded in some ordered sequence to transform the input from one form to another\n",
    "        - Rules are unambiguous\n",
    "            - Without specifying rules, we can't solve any problem in _algorithmic fashion_\n",
    "        - Sufficient Knowledge is available to fully solve the problem\n",
    "            - We need to know about the Domain to solve the problem\n",
    "- There are problems whose solutions cannot be formulated using standard rule-based algorithms\n",
    "- Problems that require subtle inputs cannot be solved using standard algorithmic approach - Face Recognition, Speech Recognition, Hand-written character recognition, etc\n",
    "- Finding Examples and using experience gained in similar situations are useful\n",
    "- Examples provide certain underlying patterns\n",
    "- Patterns give the ability to predict some outcome or help in constructing an approximate model\n",
    "- __Learning__ is the key to the ambiguous world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given the input and output, finding the relationship between Input and Output\n",
    "    - This learned one is called model\n",
    "    - So that, we can give input similar to the one given as example in learning and get the output\n",
    "- There are problems, where we can't clearly give exact details of input and the rules to find the output.\n",
    "    - In such case, we want the __*Machine*__ to __*Learn*__ from the given input (examples), to find (estimate) the Output\n",
    "- The system starts looking at the (latent) patterns found in the examples, using which it predicts/estimates the outcome\n",
    "- Always their will be new data, so _learning_ is a continuous process and model should be updated accordingly\n",
    "- From [18] Page 61\n",
    "  - _Learning_ corresponds to adjusting the values of the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How ML used in NLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification\n",
    "- Word Embedding\n",
    "- Learning a Sentence (This is not possible with Probabilistic Language Model)\n",
    "- (How to) Encode a Paragraph\n",
    "- (How to) Encode a Problem Statement\n",
    "- Translation from language to the other\n",
    "    - Can be done with Statistical Machine Translation models, but NN based models have advantage\n",
    "- Modeling conversations - Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [17]\n",
    "    - Perceptron is the basic element of Neural Network\n",
    "    - From where Neural Network started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "- Helps in answering\n",
    "    - How do we iterate to really get to the solution by descending down /ascending up the slope using Gradient Descendent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [v1] Week 3, Lec 2\n",
    "    - __Classification__ is the task of assigning predefine dis-joint categories to objects\n",
    "    - Example\n",
    "        - Detect $\\text{Spam emails}$\n",
    "        - Find the set of $\\text{mobile phones < Rs.10000 and received  $5*$ reviews}$\n",
    "            - In these kind of classification NER will be used to extract the features and then classification will be performed over the extracted features\n",
    "        - Identify the category of the incoming document as Sports, Politics, Entertainment or Business\n",
    "        - Determine whether a movie review is a Positive of Negative Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The input is a collection of records\n",
    "- Each record is represented by a tuple ($x$,$y$)\n",
    "- $x=x_1,x_2,...,x_n$ and $y=y_1,y_2,...y_n$ are the input features and the classes respectively\n",
    "- Example\n",
    "    - $x \\in R^{2}$ is a vector - the of observed variables\n",
    "    - ($x$,$y$) are related by an unknown function. The goal is to estimate the unknown function $g(.)$ also known as a classifier function, such that $g(x) = f(x), \\forall x$\n",
    "    \n",
    "    ![Classification_Model](images/Classification_Model.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the Classifier Function do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assuming we have a linearly separable $x$, the classifier function $g(.)$ implements decision rule\n",
    "    - Fitting a Straight Line to a given data set requires two parameters $(w_0 $ $and$  $w)$\n",
    "        - $w_0$ is the bias\n",
    "            - It is the distance of the line from the origin\n",
    "        - $w$ is the weight\n",
    "            - It is the orientation of the line\n",
    "        - Both $w_0$ and $w$ are called as _Model Parameters_\n",
    "        - __*Fitting the Line*__: This decision boundary line is estimated in a iterative fashion\n",
    "            - Using the errors that we are calculated after fitting a line in each iteration\n",
    "            - This fitment is learnt during the above iterative process\n",
    "    - The decision rule divides the data space into two sub-spaces separating two classes using a boundary\n",
    "    - The distance of the boundary from the origin $= \\frac{w_0}{\\parallel w \\parallel}$\n",
    "    - Distance of any point from the boundary $=d=\\frac{g(x)}{\\parallel w \\parallel}$\n",
    "    \n",
    "    ![Classifier_Function](images/Classifier_Function.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly Separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From <https://en.wikipedia.org/wiki/Linear_separability>\n",
    "    - If a set of points can be separated by using a line (a hyperplane in higher dimension), then we can say that the points are __*linearly separable*__\n",
    "= From [17] 2.4.1 Linear Separability and the XOR Problem\n",
    "  - Linear separability refers to the fact that classes of patterns with $n$-dimensional vector  ${\\bf x} = (x_1, x_2, ... , x_n)$ can be separated with a single __*decision surface*__. In below image, the line $L$ represents the _decision surface_.\n",
    "    - ![Linearly_Separable_Pattern](images/Linearly_Separable_Pattern.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal of classification is to take a vector $x$ and assign it to one of the $N$ discrete $\\mathbb{C}_n$, where $n=1,2,3,...,N$\n",
    "    - The classes are disjoint and an input is assigned to only one class\n",
    "    - The input space is divided into _decision regions_\n",
    "    - The boundaries are called a _decision boundaries or decision surfaces_\n",
    "    - In general, if the input space is $N$ dimensional, then $g(x)$ would define $N-1$ hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry of the Linear Discriminant Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [18] 2.5 DISCRIMINANT FUNCTIONS\n",
    "  - The mathematical definition of a decision boundary (the bounary line $L$) is a “discriminating function”.\n",
    "  - It is a function that maps our input features onto a classification space\n",
    "    - In the example of \"Linearly Separable\", the plane $L$ separating two clusters $x$ and $o$\n",
    "    - i.e., partitioning the _pattern space_ by _discriminant functions_\n",
    "    \n",
    "![Geomentry_of_the_Linear_Discriminant_Function](images/Geomentry_of_the_Linear_Discriminant_Function.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D-Decision Boundary for OR Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The decision regions are separated by a hyperplane and it is defined by $g(x) - 0$.\n",
    "- This separates linearly separable classes $\\mathbb{C}_1$ and $\\mathbb{C}_2$\n",
    "- The $OR$ Gate _Truth Table_\n",
    "\n",
    "| $x_1$ | $x_2$ | y |\n",
    "|-------|-------|---|\n",
    "| 0     | 0     | 0 |\n",
    "| 0     | 1     | 1 |\n",
    "| 1     | 0     | 1 |\n",
    "| 1     | 1     | 1 |\n",
    "\n",
    "- If any of the input feature $x_1$ or $x_2$ has 1, then result is $1$\n",
    "- Below diagram depicts the boundary line for this OR gate\n",
    "    ![1D_Decision_Boundary_For_OR_Gate](images/1D_Decision_Boundary_For_OR_Gate.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D-Decision Boundary for AND Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The decision regions are separated by a hyperplane and it is defined by $g(x) = 0$.\n",
    "- This separates linearly separable classes $\\mathbb{C}_1$ and $\\mathbb{C}_2$\n",
    "- The $AND$ Gate _Truth Table_\n",
    "\n",
    "| $x_1$ | $x_2$ | y |\n",
    "|-------|-------|---|\n",
    "| 0     | 0     | 0 |\n",
    "| 0     | 1     | 0 |\n",
    "| 1     | 0     | 0 |\n",
    "| 1     | 1     | 1 |\n",
    "\n",
    "- We will have 1 only when both $x_1$ and $x_2$ are 1\n",
    "- The boundary line for this $AND$ gate will similar to the one below\n",
    "\n",
    "    ![1D_Decision_Boundary_For_AND_Gate](images/1D_Decision_Boundary_For_AND_Gate.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary for Sentiments - NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The concept of decision boundary can be applied to NLP as well\n",
    "- Let us consider some positive and negative sentiment terms which are contained in two classes $\\mathbb{C}_P$ and $\\mathbb{C}_N$\n",
    "    - $\\mathbb{C}_P = [\\text{achieve efficient improve profitable}] = +1$\n",
    "    - $\\mathbb{C}_N = [\\text{termination penalties misconduct serious}] = -1$\n",
    "    \n",
    "    ![Decision_Boundary_For_Sentiments](images/Decision_Boundary_For_Sentiments.jpg)\n",
    "    \n",
    "- __*Note*__\n",
    "    - Here inputs are texts\n",
    "    - We need to transform the input for finding the decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary - Variation of $W_J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The slope (weight) of the line is decided by variations of $W_J$\n",
    "    - During fitment of a line/ learning the model, the slope of the line need to be adjusted to have _line of fit_\n",
    "    - So, $W_J$ need to be adjusted based on the errors calculated after each iteration during fitment\n",
    "\n",
    "    ![Decision_Boundary_Variation_of_W_J](images/Decision_Boundary_Variation_of_W_J.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary - Variation of Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The distance of the decision boundary from origin is decided by bias ($w_0$)\n",
    "    - During fitment, line/hyperplane need to be moved to have approximate decision boundary which splits the input to decision regions\n",
    "    - So, bias $w_0$ also need to be variated during iterative process of learning/fitment\n",
    "- The contribution of bias to the creation of the decision boundary\n",
    "\n",
    "    ![Decision_Boundary_Variation_of_Bias](images/Decision_Boundary_Variation_of_Bias.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example showing the iterative process of fitting the line using _Gradient Descent_\n",
    "    - Assume we have the following\n",
    "        - Input: 10 points are taken as input, shown in picture as $\\perp$\n",
    "        - Output: Assume output classes are well defined and well known\n",
    "    - What we don't know is how to fit the line so that decision region(s) are created to each class\n",
    "        - This fitment has to be learnt during the iterative process\n",
    "- Picture shows the fitment of line in 10 iterations\n",
    "    - Let $y$ be our target\n",
    "        - The green line which goes over $\\perp$ in the left diagram\n",
    "    - let $\\hat{y}$ is estimate\n",
    "        - The first line is shown in blue color (parallel to x-axis)\n",
    "    - Goal is to have $y-\\hat{y} \\approx 0$\n",
    "        - That is the error should reach the _minima_, or no more change that can be brought to the model parameters, then we can stop the iteration\n",
    "    - In each iteration error $y-\\hat{y}$ is calculated and it is propagated back to the model and ask the model to learn the parameter ($w$ and $w_0$ keeps changing in each iteration)\n",
    "    \n",
    "Image 1             |  Image 2\n",
    ":-------------------------:|:-------------------------:\n",
    "![Decision_Boundary_And_Gradient_Descent](images/Decision_Boundary_And_Gradient_Descent.jpg)  |  ![Decision_Boundary_And_Gradient_Descent_2](images/Decision_Boundary_And_Gradient_Descent_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Linearly Separable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is below data linearly separable?\n",
    "\n",
    "  ![Linearly_Separable](images/Linearly_Separable.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification is possible when points are __*linearly seperable*__ using linear models\n",
    "  - Using _Decision Surfaces_ where the points are _lineary separable_\n",
    "- Similarly we can do classification (linear separation) using __*Perceptron*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Biological_Neural_Network](images/Biological_Neural_Network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __*Dendrite(s)*__ provides the _input(s)_\n",
    "- __*Synapse*__ are the incoming _signals_\n",
    "- __*Axon*__ is the _output_ function\n",
    "  - Which carries the electro-chemical signal to other neurons\n",
    "  - i.e., electrical signals are carried from one neuron to another neuron using axon's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Artificial Neuron is called as __*Perceptron*__\n",
    "- From [18] Page 60\n",
    "  - Perceptron - Artificial Neuron\n",
    "  - Takes weighted sum of inputs, outputs $+1$ if greater than threshold else output $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laws of Association w.r.to NLP in Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Associative laws are useful in design of Neural Network\n",
    "  - For __*Learning and Memory*__\n",
    "    - These laws are patterns that can be used as input to the model\n",
    "    - So, we should be able to capture these properties in the text\n",
    "- __Law of Similarity__\n",
    "  - Example:\n",
    "    - In LSI\n",
    "      - From given word, find similar words of the given word (LSI)\n",
    "      - Patterns are found using this _Laws of Similarity_\n",
    "    - In word2vec\n",
    "      - Given a set of context and given the surrounding words, find the middle word (word2vec)\n",
    "- __Law of Contrast__\n",
    "  - Finding antonyms (opposite) of words from the given word\n",
    "- __Law of Contiguity__\n",
    "  - Things that link together w.r.to time / space\n",
    "- __Law of Frequency__\n",
    "  - Words that are connected through the context\n",
    "\n",
    "![Laws_Of_Association](images/Laws_Of_Association.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron vs Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                        Neuron                        |                          Perceptron                          |\n",
    "|:----------------------------------------------------:|:------------------------------------------------------------:|\n",
    "| Biological                                           | A mathematical model of a biological neuron                  |\n",
    "| Dendrites receive electrical signals                 | Perceptron receives mathematical values as input             |\n",
    "| Electro-chemical signals between Dentrites and Axons | The weighted sum represents the total strength of the signal |\n",
    "| The electro-chemical signals are not static          | Weights change during the training process                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level view of Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An example perceptron having only one element to linearly classify set of vectors to two regions\n",
    "- $x_1, x_2, ..., x_n$ are called _inputs_ or _feature vectors_, connected to the perceptron using weights $w_1, w_2, ..., w_n$\n",
    "  - The input could be\n",
    "    - Any real values\n",
    "    - Or a Ont-Hot Encoded Vectors in case of NLP\n",
    "- $+1$ is the bias, connected to the preceptron using weight $w_0$\n",
    "- $\\hat{y}$ is the output is going to be two values\n",
    "  - $\\{-1,+1\\}$ depending on value of activation function\n",
    "- For the sake of explanation, perceptron is shown as _Activation Function_ and _Decision Function_\n",
    "  - Activation Function\n",
    "    - Once the values are received, they are linearly sum'ed\n",
    "    - We will have value, for example between $[-5,+5]$ depending on the weights that are connecting the neuron\n",
    "    - Activation Function value need to translated into a real value range $[0,1]$ or $[-1,+1]$ or using some probability distribution, where sum of all values equal 1 $\\sum [0..1] = 1$\n",
    "    - What Activation function does is, it smashes/squashes the calculated neuron value into new space $[0,1]$\n",
    "  - Decision Function\n",
    "    - Translates the _Activation Function_ value into two different values using some __*threshold*__\n",
    "\n",
    "![High_Leve_View_of_Perctron](images/High_Leve_View_of_Perctron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do we teach perceptron or how does percetron learn the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perceptron learns the weights\n",
    "  - Weights are the model parameters.\n",
    "  - So, it needs to learn the weights\n",
    "- They are adjusted until the output is consistent with the target output in the training examples\n",
    "- Let $k$ be the number of iterations went through in perceptron learning process so far\n",
    "  - So we will have $w^1, w^2, ..., w^k$ weights learned in each iteration\n",
    "- $w^{k+1} \\propto (y - \\hat{y})$\n",
    "  - The new weight $w^{k+1}$ is proportional to the errors $(y - \\hat{y})$ that were computed\n",
    "    - $y$ is actual target of the training data\n",
    "    - $\\hat{y}$ is the estimated output\n",
    "- The weights are updated as below\n",
    "  - $w^{k+1}_j = w^{(k)}_j - \\eta(y_i - \\hat{y}^{(k)})x_{ij}$\n",
    "    - where\n",
    "      - $w^{(k)}$ is the weight parameter associated with the $i^{th}$ input at $k^{th}$ iteration\n",
    "      - $\\eta$ is the learning parameter\n",
    "        - It is the step size, helping in how to descent, to find target output\n",
    "          - If $\\eta$ value is very high, the learning jumps\n",
    "          - If $\\eta$ value is very small, it slowly and steadily reaches the target output\n",
    "          - This $\\eta$ value will be decided/adjusted based on\n",
    "            - Input features\n",
    "            - Training samples\n",
    "            - How weights are adjusted in each iteration\n",
    "            - How the errors are jumped from one point to the other in each iteration\n",
    "          - This $\\eta$ parameter is updated based on the experienced that we gain on model estimation\n",
    "        - Normally it ranges from $[0.1, 0.01]$\n",
    "      - $x_{ij}$ is the $j^{th}$ attribute of the $i^{th}$ training sample\n",
    "  - The new weight $w^{k+1}_j$ is given by old weight $w^{(k)}_j$, learning parameter $\\eta$, the error $(y - \\hat{y})$ and the input parameter $x_{ij}$\n",
    "- If $(y - \\hat{y}) \\approx 0$, no prediction error\n",
    "  - Normally it will be set to $1.e-5$\n",
    "- During the training the weights contributing most to the error require adjustments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets suppose\n",
    "  - Target output is\n",
    "    - $y = 1$\n",
    "  - The Estimated output is\n",
    "    - $\\hat{y} = -1$\n",
    "- How will you update $w$?\n",
    "- What kind of adjustment you will make to $w$ so that $\\hat{y}$ becomes closer to $y$?\n",
    "\n",
    "- Refer some books and figure out what could the adjustments that we can make?\n",
    "  - Hint: $w^{k+1} \\propto (y - \\hat{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Same of Exercise 3, but\n",
    "  - $y = -1$\n",
    "  - $\\hat{y} = 1$\n",
    "- In which way you would adjust the weights?\n",
    "  - Either you will increase the weight or you will decrease the weight?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm for Perceptron Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Total number of input vectors = $k$\n",
    "2. Total number of features = $n$ (for each vector)\n",
    "3. Learning parameter $\\eta = 0.001$, where $0 < \\eta < 1$\n",
    "4. $epoch^1$ count $t = 1$, $j = 1$\n",
    "5. Initialize weights $w_i$ with random numbers\n",
    "6. Initialize the input layer with $\\vec{x_j}$\n",
    "7. Calculate the output using $\\sum w_i x_i + w_0$\n",
    "8. Calculate the error $(y- \\hat{y})$\n",
    "9. Update the weights $w_j(t + 1) = w_j - \\eta(y-\\hat{y})x_j$\n",
    "10. Repeat steps 7 to 9 until: the error is less than $\\theta$ (the given threshold) or a predetermined number of $epochs$ have been completed\n",
    "\n",
    "$^1$An epoch is one complete presentation of the data set to be learned to a learning machine\n",
    "\n",
    "![Algorithm_for_Perceptron_Learning](images/Algorithm_for_Perceptron_Learning.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From Algorithm for Perceptron Learning\n",
    "  - To provide a stable weight update for this step, $w_j(t + 1) = w_j - \\eta(y-\\hat{y})x_j$, we require a small $\\eta$. This results in slow learning. Bigger $\\eta$ would be good for fast learning.\n",
    "    - What are the problems?\n",
    "    - What is the compromise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can be done with Perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You will be able to classify objects that are in linear space\n",
    "  - It is __*mandatory*__, that the objects that we are going to classify should be __*linearly separable*__ ![Perceptron_Linearly_Separable_Requirement](images/Perceptron_Linearly_Separable_Requirement.jpg)\n",
    "- Perceptron will contain __*only one neuron*__, even though we have $n$ number of neurons in input layer\n",
    "  - Input layer neurons just pass on the value to the perceptron\n",
    "  - The computation will happen only in Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - How Perceptron behaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical AND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here for demonstration purpose, below values has been set to:\n",
    "  - $b = -1, w_0 = -1$\n",
    "  - $w_1 = +1$ for $x_1$\n",
    "  - $w_2 = +1$ for $x_2$\n",
    "- With above learnt model parameters, it is evident that Perceptron able to classify the linearly separable objects\n",
    "\n",
    "![Perceptron_Learning_Logical_AND](images/Perceptron_Learning_Logical_AND.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical OR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here for demonstration purpose, below values has been set to:\n",
    "  - $b = +1, w_0 = 0$\n",
    "  - $w_1 = +1$ for $x_1$\n",
    "  - $w_2 = +1$ for $x_2$\n",
    "- With above learnt model parameters, it is evident that Perceptron able to classify the linearly separable objects\n",
    "\n",
    "![Perceptron_Learning_Logical_OR](images/Perceptron_Learning_Logical_OR.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis - Using Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Source - <https://nlp.stanford.edu/projects/glove>\n",
    "- For example, to do a +ive/-ive review classification, we need\n",
    "  - Word Embeddings as Input features\n",
    "    - Above data source provides word embedding with 50, 100, 300, ... vector sizes\n",
    "    - Ensure that the word that we are taking are present in the word embedding. ONE of the most important thing that we need to do.\n",
    "  - We need +ive and -view words for training\n",
    "\n",
    "![Sentiment_Analysis_Using_Perceptron](images/Sentiment_Analysis_Using_Perceptron.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Training Data\n",
    "\n",
    "- Code to generate the training data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def generate_data():\n",
    "    #data from https://nlp.stanford.edu/projects/glove/\n",
    "    #...\n",
    "    #...\n",
    "    for pos_word in positives:\n",
    "        positive_words.append[post_word.rstrip()]\n",
    "\n",
    "    for neg_word in negatives:\n",
    "        negative_words.append[neg_word.rstrip()]\n",
    "\n",
    "    for line in glove:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        if word in positive_words:\n",
    "            vector = np.append(vector,[1.0])\n",
    "            emb_dict[word] = vector\n",
    "        elif word in negative_words:\n",
    "            vector = np.append(vector, [0.0])\n",
    "            emb_dict[word] = vector\n",
    "\n",
    "    #...\n",
    "    \n",
    "    dump(emb_dict, data_dir, 'SentiWordEmbedding.bin')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def combine_input_and_weights(self, X):\n",
    "    # linearly combine input vectors and weight vectors\n",
    "    return np.dot(X, self.weights)\n",
    "\n",
    "def build_model(self, X, y):\n",
    "    # Build a model using the training data X and the class associated with each word embedding\n",
    "    # X contains the word embeddings of sentiment words\n",
    "    # y array contains the sentiment lables for every word - positive = 1, negative = 0\n",
    "    X = self.normalize_feature_values(X)\n",
    "    self.initialize_weights(X)\n",
    "    for i in range(self.epochs):\n",
    "        predicted_output = self.activate_function(self.combine_input_and_weights(X))\n",
    "        errors = y - predicted_output\n",
    "        self.weights += (self.eta * X.T.dot(errors))\n",
    "        # Comput the cost function\n",
    "        cost_function = (errors ** 2).sum() / 2.0\n",
    "        self.cost.append(cost_function)\n",
    "    return self\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def predict(self, X):\n",
    "    # predict the output corresponding to the input vector X\n",
    "    X = self.normalize_feature_values(X)\n",
    "    return np.where(self.activate_function(self.combine_input_and_weights(X)) >= 0.0, 1, 0)\n",
    "\n",
    "classifier = Perceptron(eta=0.00001, epoch=5000)\n",
    "classifier.build.model(np.array(X),np.array(y))\n",
    "\n",
    "test = sent_embedding_dict['terrible']\n",
    "sentiment = classifier.predict(X_test)\n",
    "print(sentiment) # 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Learning through EPOCH count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we do the iteration through the $epoch$ count\n",
    "  - we can find that the _decision boundary_ keep shifting between values\n",
    "\n",
    "\n",
    "  Image 1             |  Image 2\n",
    ":-------------------------:|:-------------------------:\n",
    "!![Perceptron_Learning_through_EPOCH_count](images/Perceptron_Learning_through_EPOCH_count.jpg)  |  ![Perceptron_Learning_through_EPOCH_count_2](images/Perceptron_Learning_through_EPOCH_count_2.jpg)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is based on the linear combination of fixed basis functions\n",
    "- Updates the model only based on misclassification\n",
    "- Documents that are linearly separable are classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logical XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since Logical XOR data are not linearly separable, it is not possible to solve this problem using _Perceptron_\n",
    "- How __*Logical XOR*__ can be solved?\n",
    "  - We need to figure out a way to solve the problems, where\n",
    "    - The boundaries cannot be a straight line\n",
    "    - We cannot separate the classes by just a straight line\n",
    "- In Non-linear cases, we need to increase the hidden units (it depend on the size of the application)\n",
    "- Refer [17] to know more on how hidden layer solves this XOR problem\n",
    "- [v3] provides good explanation on soft-boundary\n",
    "- From [18]\n",
    "  - 2.7 LINEAR CLASSIFIERS Page  31\n",
    "    - Refer to get intuition on decision boundary in a __*piecewise fashion*__\n",
    "  - 3.6 LIMITATIONS OF PERCEPTRONS \n",
    "    - to get more details on XOR\n",
    "  - Page 13\n",
    "    - Visually showing how Multi-layer perception solves this XOR problem\n",
    "      - ![Perceptrons_Solving_XOR_Problem](images/Perceptrons_Solving_XOR_Problem.jpg)\n",
    "\n",
    "|                   Image 1                  |                   Image 2                  | Image 3                                    |\n",
    "|:------------------------------------------:|:------------------------------------------:|--------------------------------------------|\n",
    "| ![Logical_XOR_1](images/Logical_XOR_1.jpg) | ![Logical_XOR_2](images/Logical_XOR_2.jpg) | ![Logical_XOR_3](images/Logical_XOR_3.jpg) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input space is transformed into hidden space\n",
    "- Hidden layer represents the input layer\n",
    "- Learns automatically the input representation and patterns\n",
    "- $(0,1)$ and $(1,0)$ are merged into one in the $h$-space\n",
    "- Patterns yielding similar results are merged into one\n",
    "- Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add one more neuron in the hidden layer and compute the output matrix\n",
    "  - See how values are reshaped in hidden layer and how it impacts the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Are hidden layer neurons joining piecewise linear representations to create non-linear-boundaries?\n",
    "  - Hint: Refer [18] 2.7 LINEAR CLASSIFIERS Page 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Activation functions are one of the most important aspect of Neural Network\n",
    "  - In order to get a very stable weight, we need to have a _very good activation function_\n",
    "  - A good activation function means that it is _differentiable_\n",
    "    - If the activation function is not differentiable, then it is not possible to do the learning in NN\n",
    "- There are several activation functions available\n",
    "  - Hard threshold\n",
    "  - Sigmoid (normally used in hidden layers)\n",
    "  - Tanh (normally used in hidden layers)\n",
    "  - ReLu - Rectified Linear Unit\n",
    "  - Leaky ReLu\n",
    "  - Softmax (Popular in NLP, normally used in the last layer of NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Value beyond a point (threshold) is considered $1$, otherwise it is considered as $0$\n",
    "  - ![Hard_Threshold](images/Hard_Threshold.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The sigmoid is a _non-linear function_\n",
    "- Better than hard threshold function as it __*squashes the net output into the range $[0,1]$*__\n",
    "- The values closer to the tails become $0$ or $1$\n",
    "- In some cases, the values quickly saturate at $0$ or $1$\n",
    "- At the bottom tail, most values become zero during the training and hence the most important aspect of learning of neural network is inhibited\n",
    "- Sigmoid output are not zero-centered - $[0,1]$\n",
    "- It is undesirable to have all the values squashed near the tails, where the gradient is $0$\n",
    "  - If slope (gradient) gets zero, then there is no more learning\n",
    "    - In $w_j(t + 1) = w_j - \\eta(y-\\hat{y})x_j$\n",
    "      - $(y-\\hat{y})x_j$ becomes zero, no more learning\n",
    "    - It implies there is some issue either in Input or in weight assignment or in NN layer architecture\n",
    "    - You need to check and fix it\n",
    "  - That is, we need to some difference $(y-\\hat{y})x_j$ between previous iteration and this iteration to have a learning.\n",
    "    - If this difference (slope) becomes zero, we cannot learn anymore\n",
    "  - ![Sigmoid_Activation_Function_Range](images/Sigmoid_Activation_Function_Range.jpg)\n",
    "- Various kind of Sigmoid functions exists\n",
    "  - Irrespective of the kind of sigmoid, we do not want the one having very long tail\n",
    "    - Because the values will get saturated quickly to $0$ or $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## TanH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is a hyperbolic Tangent activation function\n",
    "- This is a zero based non-linear function\n",
    "  - The range is between $[-1,+1]$\n",
    "- This is most common in RNN (Recurrent Neural Network), in hidden layers\n",
    "- $(\\frac{e^z - e^{-z}}{e^z + e^{-z}})$\n",
    "\n",
    "![TanH_Activation_Function](images/TanH_Activation_Function.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU - Rectified Linear Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ReLU](images/ReLU.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function - Python Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(X,W,b):\n",
    "    return 1.0/(1.0 + np.exp(-(np.dot(W.T,X) + b)))\n",
    "\n",
    "def tanh(X,W,b):\n",
    "    z = np.exp(-(np.dot(W.T,X) + b))\n",
    "    return (np.exp(z) - np.exp(-z))/(np.exp(z) + np.exp(-z))\n",
    "\n",
    "def relu(X,W,b):\n",
    "    x = np.dot(W.T,X) + b\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def softmax(X,W,b):\n",
    "    z_exp = np.exp(np.dot(W,X)+ b)\n",
    "    z_exp_sum = np.sum(z_exp)\n",
    "    return z_exp/z_exp_sum\n",
    "\n",
    "W = np.array([0.1, 0.2, 0.6])\n",
    "X = np.array([0.2, 0.1, 0.3])\n",
    "b = 1.5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Decision Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All linear classifier are used for binary classification\n",
    "- In NLP problems, we need to identify more than two classes\n",
    "  - Document classification\n",
    "  - Sentiment Analysis\n",
    "    - Positive, Negative, Neutral and Non-sentiment word (i.e., the document doesn't belong to any of the categories, possibly an outlier)\n",
    "- We need a decision function that predicts more than two classes by providing appropriate values\n",
    "- An extension of the case function would be hard to manage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Need a function that takes as input a vector of size $N$ real numbers, and normalizes it into a $K$ classes\n",
    "- Need a function that normalizes the net output and classes well separated (ideal condition)\n",
    "- Need a function that fits the classes using probability and distributes the probability density\n",
    "\n",
    "$Softmax(a_j) = P(C_k | x_j)$ $=$ $\\frac{e^{a_j}}{\\sum_{j=1}^K e^{a_k}}$\n",
    "\n",
    "![Softmax](images/Softmax.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax - Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def softmax(X,W,b):\n",
    "  z = np.exp(np.dot(W,X)+b)\n",
    "  return z/np.sum(z)\n",
    "\n",
    "W = np.array([0.1, 0.2, 0.5])\n",
    "X = np.array([0.2, 0.1, 0.3])\n",
    "b = 1.5\n",
    "W = np.array([[1,2,3], [2,3,8], [1,5,7]])\n",
    "\n",
    "print(softmax(X,W,b)) # [0.08672022 0.52462674 0.38865305]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function (Error Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [18] 2.7 LINEAR CLASSIFIERS Page 30\n",
    "  - The _error function_ typically compares the output of the classifier with a desired response and gives an indication of the difference between the two.\n",
    "\n",
    "![Loss_Function](images/Loss_Function.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cost_Function](images/Cost_Function.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Descent is usually used __*to get to the minimum*__\n",
    "  - Mostly we want to get into __*global minimum*__\n",
    "\n",
    "![Gradient_Descent](images/Gradient_Descent.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradient_Descent_Advantages](images/Gradient_Descent_Advantages.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many layers in NN needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [18]\n",
    "  - Read whole 4.8 MULTILAYER PERCEPTRONS AS CLASSIFIERS \n",
    "    - __*Kolmogorov Theorem*__\n",
    "      - Three layers of perceptron units can therefore form arbitrarily complex shapes, and are capable of separating any classes\n",
    "      - _The complexity of the shapes is limited by the number of nodes in the network_\n",
    "        - Since these define number of edges that we can have.\n",
    "      - __*The arbitrary complexity of shapes that we can create, means that we never need more than three layers in a network*__, a statement that is referred to as the __*Kolmogorov Theorem*__\n",
    "    - Arbitrary_Regions\n",
    "      - ![Arbitrary_Regions](images/Arbitrary_Regions.jpg)\n",
    "    - Perceptron_Classification_Abilities\n",
    "      - ![Perceptron_Classification_Abilities](images/Perceptron_Classification_Abilities.jpg)\n",
    "    - Boundaries_Formed_For_Arbitrary_Regions\n",
    "      - ![Boundaries_Formed_For_Arbitrary_Regions](images/Boundaries_Formed_For_Arbitrary_Regions.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
