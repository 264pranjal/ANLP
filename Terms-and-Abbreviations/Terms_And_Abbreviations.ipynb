{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terms and Abbreivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measures\n",
    "\n",
    "### Proximity Measure\n",
    "\n",
    "A proximity[1] is a number which indicates how similar or how different two objects are, or are perceived to be, or any measure of this kind\n",
    "\n",
    "### Tanimoto Coefficient\n",
    "\n",
    "It is a similarity measure[2], can be applied to vectors having binary values as attributes.\n",
    "\n",
    "Mostly Used in fingerprints[3] based similarity check. Especially in Cheminformatics.\n",
    "\n",
    "Tanimoto Coefficient[4] value between 0 and 1, with 1 corresponding to identical fingerprints, i.e. protein–ligand interaction patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI - Latent Semantic Indexing\n",
    "\n",
    "LSI [5] compares how often words appear _together in the same document_ and compares how often those occurences happen in _ALL_ of the documents that Google has in its index\n",
    "[6] Nice Explanataion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability [7]\n",
    "\n",
    "Probability is defined as the likelihood that an event will occur\n",
    "\n",
    "Eg., Flipping a coin. There is a 50% chance or probability that heads will come up for any given toss of a fair coin.\n",
    "\n",
    "Probability can be expressed as\n",
    "\n",
    "- as a Percentage - Eg., 60%\n",
    "- as a Decimal Form - Eg., 0.6\n",
    "- as a Fraction - 6/10\n",
    "\n",
    "### Probability in NLP\n",
    "\n",
    "#### Why Probability used in NLP\n",
    "- Probability will be used in estimating what could be the next word in the sentence\n",
    "- Provides methods to predict or make decisions to pick the next word in the sequence based on sampled data\n",
    "- Make the informed decision when there is a certain degree of uncertainty and some observed data\n",
    "    - Example: How * you?\n",
    "    - Finding all the possible words that might appeat in between How and you\n",
    "    - To get an understanding, see Google NGram Viewer\n",
    "- It provides a quantitative description of the chances or likelihood's associated with various outcomes\n",
    "\n",
    "#### How Probability used in NLP\n",
    "\n",
    "1. Probability of a Sentence\n",
    "    - Probability of the next word in the sentence?\n",
    "        - How likely to predict \"you\" as the next word after the query sentence \"How are ____?\"\n",
    "            - Likelihood of the next word is formalized through an observation by conducting experiment - counting the words in a document\n",
    "            \n",
    "#### Discrete Sample Space\n",
    "- Consider the following Bag of Words (_count = 52_)\n",
    "    - Experiment\n",
    "        - Extracting tokens from a document\n",
    "    - Outcome\n",
    "        - Every token/word in _x_ in the document\n",
    "    - Sample Document\n",
    "        - A weather balloon is floating at a constant height above Earth when it releases a pack of instruments. (Level 1) a. If the pack hits the ground with a downward velocity of −73.5 m/s, how far did the pack fall? b. Calculate the distance the ball has rolled at the end of 2.2 s \n",
    "- The outcome of the experiment - 52 sample (words).\n",
    "    - They constitute the _sample space_, $\\Omega$ or the set of all possible outcomes\n",
    "        - $\\Omega$ = 'a', 'weather', 'balloon', 'is', 'floating', 'at', 'a', 'constant', 'height', 'above', 'earth', 'when', 'it', 'releases', 'a', 'if', 'the', 'pack', 'hits', 'the', 'ground', 'with', 'a', 'downard', 'velocity', 'of', 'm', 's', 'how', 'far', 'did', 'the', 'pack', 'fall', 'b', 'calculate', 'the', 'distance', 'the', 'ball', 'has', 'rolled', 'at', 'the', 'end', 'of', 's'\n",
    "- Each word in this sample belongs to $\\Omega$, represented by $x \\in \\Omega$\n",
    "- Eacm sample $x \\in \\Omega$ is assigned a probability score $[ 0, 1 ]$\n",
    "\n",
    "#### Probability Mass Function\n",
    "- Probability Function | Probability Distribution Function\n",
    "    - A _probability function_ or _probability distribution function_ distributes the probability mass of $1$ to the all the samples in the sample space $\\Omega$\n",
    "\n",
    "#### Sample Space Constraints\n",
    "- All the words in the $\\Omega$, must satisfy the following constraints:\n",
    "    1. $P(x) \\in [0,1], for all x \\in \\Omega$\n",
    "    2. $\\sum_{x \\in \\Omega} P(x) = 1$\n",
    "\n",
    "#### Events\n",
    "\n",
    "- Events can be described as a variable taking a certain value\n",
    "- An __*Event*__ is a collection of samples of the same type, $E \\subseteq \\Omega$\n",
    "    - $P(E) = \\sum_{x \\in E} P(x)$\n",
    "- Example\n",
    "    - Consider above sample document\n",
    "        - Total number of words = 52.\n",
    "        - The number of _uniqye_ words = 37 or there are 37 __*types*__ of words in this BOW.\n",
    "        - 15 words have frequencies $> 1$.\n",
    "\n",
    "#### Random Variable\n",
    "\n",
    "- A __random variable__[8], is a variable whose possible values are numerical outcomes of a random experiment\n",
    "- Two types of random variable\n",
    "    - Continuous\n",
    "    - Discrete\n",
    "- For NLP, it will be __*Discrete*__\n",
    "\n",
    "- To capture Type-Token distinction, we use random variable $W$.\n",
    "    - $W(x)$ maps to the sample $x \\in \\Omega$\n",
    "- $V$ is the set of types and the value is represented by a variable $v$\n",
    "- Given a random variable $V$ and a value $v$, $P(V = v)$ is the probability of the event that $V$ takes the value $v$, i.e: $P(V = v) = P(x \\in \\Omega: V(x) = v)$\n",
    "    - Example: $P(V = 'the') = P('the') = 0.115$\n",
    "- Random variables are useful in describing/ constructing various events\n",
    "\n",
    "#### Probability of the sentence $W$:\n",
    "\n",
    "> $P(W) = P(w_1, w_2, ..., w_n)$\n",
    "\n",
    "#### Chain Rule:\n",
    "\n",
    "> $P(w_1, w_2, ..., w_n) = P(w_1)P(w_2|x_1)...P(w_n|w_1,...w_{n-1})$\n",
    "\n",
    "> $P('I \\; got \\; this \\; one') = P('I', 'got', 'this', 'one'')$\n",
    "\n",
    "> $P('I \\; got \\; this \\; one') = P('I') × P('got' | 'I') × P('this' | 'I \\;  got') × P('one' | 'I \\;  got \\;  this')$\n",
    "\n",
    "#### Markove Assumption:\n",
    "\n",
    ">  $P('I \\; got \\; this \\; one') = P('I') × P('got' | 'I') × P('this' | 'got') × P('one' | 'this')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] <https://www.leydesdorff.net/aca/>\n",
    "\n",
    "[2] <http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/sphilip/tani.html>\n",
    "\n",
    "[3] <https://www.surechembl.org/knowledgebase/84207-tanimoto-coefficient-and-fingerprint-generation>\n",
    "\n",
    "[4] <https://jcheminf.biomedcentral.com/articles/10.1186/s13321-018-0302-y>\n",
    "\n",
    "[5] <https://www.youtube.com/watch?v=LOPY1hPcZEM>\n",
    "\n",
    "[6] <https://www.youtube.com/watch?v=OvzJiur55vo>\n",
    "\n",
    "[7] <https://cs.brown.edu/courses/cs146/assets/files/langmod.pdf>\n",
    "\n",
    "[8] http://www.stats.gla.ac.uk/steps/glossary/probability_distributions.html\n",
    "\n",
    "\n",
    "- Markdown Symbols Help\n",
    "    - <https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html>\n",
    "    - <https://csrgxtu.github.io/2015/03/20/Writing-Mathematic-Fomulars-in-Markdown/>\n",
    "    - <https://github.com/Jam3/math-as-code/blob/master/README.md>\n",
    "    \n",
    "- Jupyter Notebook Markdown Help\n",
    "    - <https://www.ibm.com/support/knowledgecenter/en/SSGNPV_1.1.3/dsx/markd-jupyter.html>\n",
    "    - <https://nbviewer.jupyter.org/github/ipython/ipython/blob/2.x/examples/Notebook/Display%20System.ipynb#LaTeX>\n",
    "    - <https://www.math.ubc.ca/~pwalls/math-python/jupyter/latex/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
