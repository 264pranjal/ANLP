{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Backpropagation Through Time (BPTT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [v1] Lecture 57\n",
    "  - This session covers\n",
    "    - Back Propagataion Through Time\n",
    "      - Derivates of Activation Functions\n",
    "      - Perplexity\n",
    "      - Exploding/Vanishing Gradient\n",
    "      - Gradient Clipping - A technique to solve Exploding/Vanishing Gradient problem\n",
    "    - LSTM(Long Short Term Memory) - a new mechanism by which we can solve certain problems exposed by RNN\n",
    "      - Introduction\n",
    "      - LSTM Cell\n",
    "      - LSTM Forward Pass\n",
    "      - Truncated  BPTT\n",
    "      - Kinematics Problem Generation\n",
    "    - GRU (Gated Recurrent Unit) - Another modification of RNN\n",
    "      - Introduction\n",
    "      - GRU Forward Pass\n",
    "- Need to read lot more to understand this derivation (lecture info is not enough)\n",
    "  - Latest research papers published in these topics\n",
    "  - E.g., on how Exploiding/Vanishing Gradient problem is addressed in Backpropagation through time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives of Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some of Activation Functions\n",
    "  - $\\large y =$ $\\large Sigmoid$ = $\\large \\sigma =$ $\\large \\left( \\dfrac{1}{1+{e^{-x}}} \\right)$ - A sigmoid activation function\n",
    "  - $\\large y =$ $\\large tanh(x)$ = $\\large \\dfrac{sinh}{cosh}$ - A hyperbolic tangent activation function\n",
    "- Some Error Functions\n",
    "  - $\\large E =$ $\\large -ylog(\\hat{y})$ - Cross Entrophy \n",
    "  - $\\large E =$ $\\large \\dfrac{1}{2} \\sum_j {\\left( y_j - \\hat{y_j} \\right)}^2$ - Mean Squared Error\n",
    "- Activations functions should be\n",
    "  - Continuous in nature\n",
    "  - They are differentiable\n",
    "    - If we don't have a diffentiable function, we cannot use that as a Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_BPTT_Derivatives_Of_Activation_Functions](images/RNN_BPTT_Derivatives_Of_Activation_Functions.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives of Sigmoid and TanH activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_Derivatives_of_Sigmoid_TanH](images/RNN_Derivatives_of_Sigmoid_TanH.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass - Network Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $D_w$ represtns the input layer\n",
    "  - Input can be a Word Vector or One-Hot-Vector\n",
    "    - If it is Word Vector, then the lenght of $D_w$ will be the size of the word vector that we have selected.\n",
    "      - Normally it is 50, 100, or 300 which are available from both Word2Vec and GloVe\n",
    "    - If it is One-Hot-Vector, then the length of $D_{|v|}$ will be the size of the vocabulary in the corpus\n",
    "- $D_h$ represents the number of neurons in the hiden layer\n",
    "- $U$ matrix represnts the values of hidden unit to the hidden unit\n",
    "  - It is a square matrix\n",
    "  - It will be of size $D_h \\times D_h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_BPTT_Forward_Pass_Network_Equations](images/RNN_BPTT_Forward_Pass_Network_Equations.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of the RNN Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Following example depicts how big a RNN network will be\n",
    "  - Assume we are using Word Vectors, which are of size $100$\n",
    "  - The number of hidden neurons is $500$\n",
    "  - We are having only one hidden layer\n",
    "  - The vocabulary size is $|V| = 10000$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_BPTT_Size_of_the_RNN_Network](images/RNN_BPTT_Size_of_the_RNN_Network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT - Derivative for V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See [v1] Lecture 58\n",
    "- $\\large \\delta_{out}^t$ is the loss for each of the units in the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_BPTT_Derivative_For_V](images/RNN_BPTT_Derivative_For_V.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT - Derivative for W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See [v1] Lecture 58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_BPTT_Derivative_For_W](images/RNN_BPTT_Derivative_For_W.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT - Derivative for U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See [v1] Lecture 58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_BPTT_Derivative_For_U](images/RNN_BPTT_Derivative_For_U.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT - Unrolled RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- See [v1] Lecture 58"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_BPTT_Unrolled_RNN](images/RNN_BPTT_Unrolled_RNN.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When do we stop our RNN Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Error $E(\\theta)$\n",
    "  - Once our RNN network settles we stop, that is\n",
    "    - When the error becomes small (that is within our acceptable range), Or\n",
    "    - When the error does not change beyond a point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When the error becomes smaller and smaller, the network has lot of confidence in identifying/predicting the next word (if we are using it for NLP application)\n",
    "  - The $E(\\theta)$ is known as Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_Perplexity_1](images/RNN_Perplexity_1.jpg)\n",
    "![RNN_Perplexity_2](images/RNN_Perplexity_2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
