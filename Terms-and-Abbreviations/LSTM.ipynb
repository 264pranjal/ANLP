{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM - Long Short Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [v1] Lecture 60\n",
    "  - LSTM, another variation of RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Study Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [An empirical exploration of recurrent network architectures](https://dl.acm.org/citation.cfm?id=3045367)\n",
    "- https://dblp.uni-trier.de/db/journals/corr/corr1506.html\n",
    "  - [A Critical Review of Recurrent Neural Networks for Sequence Learning](https://arxiv.org/pdf/1506.00019.pdf)\n",
    "- [Deep Learning](https://web.cs.hacettepe.edu.tr/~aykut/classes/spring2018/cmp784/slides/lec7-recurrent-neural-nets.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The component of the gradient in directions that correspond to long-term dependencies is [small](https://dl.acm.org/citation.cfm?id=3045118.3045367)\n",
    "  - From state $t$ to state $0$\n",
    "- The components of the gradient in directions that correspond to short-term dependencies are large\n",
    "- As a result, RNNs can easily learn the short-term but not the long-term dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In LSTM network, the network is the same as a standard RNN, except that the summation units in the hidden layer are replaced by memory blocks\n",
    "  - This will be done in $\\large s_t$\n",
    "- The multiplicative gates allow LSTM memory cells to store and access information over periods of time, thereby mitigating the [vanishing gradient problem](https://dblp.uni-trier.de/db/journals/corr/corr1506.html)\n",
    "- Along with the hidden state vector $\\large h_t$, LSTM maintains a memory vector $\\large C_t$\n",
    "  - $\\large \\large h_{t} = \\text{tanh} \\left(Uh_{t-1}+ W {x_{t}} \\right)$ going to be replaced with $\\large C_t$\n",
    "  - $\\large C_t$ is going to tell us how to condition the values of $\\large U$, so that _vanishing gradient problem disappears_\n",
    "- At each time step, the LSTM can choose to read from, write to, or reset the cell using explicit gating mechanisms\n",
    "  - A small computer kind of logic exist inside, which is able to read, write and reset operations, so that $\\large h$ values are very well conditioned\n",
    "- LSTM computes well behaved gradients by controlling the values using the gates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below diagram shows how does a LSTM Cell look like\n",
    "  - $\\large h_t$ is replaced by this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM_Cell](images/LSTM_Cell.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
