{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Refer [RNN-in-NLP.ipynb](RNN-in-NLP.ipynb) for more information on how RNN is used/applied in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sequential data prediction is considered as a key problem in machine learning and artificial intelligence\n",
    "- Unlike images where we look at the entire image, we read text documents sequentially to understand the conte\n",
    "- The likelihood of any sentence can be determined from everyday use of language\n",
    "- The earlier sequence of words (in time) is important to predict the next word, sentence, paragraph or chapter\n",
    "- If a word occurs twice in a sentence, but could not be accomodated in the sliding window, then the word is learned twice\n",
    "- __*An architecture that does not impose a fixed-length limit on the prior context*__ - _The Key Aspect Of RNN_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __States__ are important in the reading exercise. The previous state definitely affects the next state\n",
    "- In order to use the previous stat, we need to store it or remember it\n",
    "- Traditional Neural Networks were not designed as a state machine as anything\n",
    "- Traditional Neural Networks do not accept arbitrary input length\n",
    "- Inherent ability to model sequential input\n",
    "- Handle variable lenght inputs without the use of arbitrary fixed-sized windows\n",
    "- Use its own output as input\n",
    "- __RNN__s encode not only __attributional similarities between words__, but also __similarities between pairs of words item Analogy__\n",
    "  - i.e., it finds out the analogy\n",
    "    - $\\texttt{Chennai : Tamil :: London : English}$\n",
    "    - $\\texttt{go and went}$ is same as $\\texttt{run and ran}$\n",
    "    - $\\texttt{queen} \\approx \\texttt{king - man + woman}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN is almost similar to Traditional NN, except that there is a recurrent part ($\\large h_t$)\n",
    "  - A small loop in the hidden layer part\n",
    "- Why a loop in hidden layer\n",
    "  - We want to capture the preivous state $\\large h_{t-1}$, which we have calcualted from input as a linear combination, and include this as part of the new state $\\large h_t$\n",
    "  - i.e., we are incorporating whatever learned in the embedding layer as part of the current state\n",
    "- $\\large h_t$ is going to give the _time-series_\n",
    "\n",
    "![RNN_A_Simple_RNN](images/RNN_A_Simple_RNN.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN - An Extension of a Feed Forward Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A new parameter $\\large U$ is introduced, which has the previous state of embedding layer\n",
    "  - It is a hidden layer to hidden layer parameter\n",
    "  - Previous state $\\large h_{t-1}$ is stored as $\\large U$ (all the time slice info maintained, not just last time slice hidden layer activation values)\n",
    "  - It is the memory part of RNN, which we want to retain\n",
    "- In RNN, we need to learn $\\large V, W, U$, 3 parameters need to be learnt\n",
    "\n",
    "![RNN_An_Extension_of_a_Feed_Forward_Network](images/RNN_An_Extension_of_a_Feed_Forward_Network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Architectures of RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With the capability of memory in RNN, Neural Network can be build/create in sevaral ways\n",
    "  - __One-to-One__\n",
    "    - Use the neural network as a standard neural network for classification purposes\n",
    "  - __One-to-Many__\n",
    "    - Can be used for image description\n",
    "      - Input is single image\n",
    "      - Output is description of scene in that image\n",
    "  - __Many-to-One__\n",
    "    - Basically used in Sentiment Analysis\n",
    "  - __Many-to-Many__\n",
    "    - Used for Machine Translation (right corner image in the diagram show below)\n",
    "      - Given a set of words (sentence), translate it into another language have a set of words\n",
    "    - Used for frame labeling of video sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_Multiple_Architectures_Of_RNN](images/RNN_Multiple_Architectures_Of_RNN.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below agorithm shows the Feed Forward flow of RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_Feed_Forward_Algorithm](images/RNN_Feed_Forward_Algorithm.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Various representation of 'Recurrent Neuron' appears in various research papers\n",
    "- Below is one of the representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Representation 1 | Representation 2 |\n",
    "| ---- | ---- |\n",
    "| ![RNN_Recurrent_Neuron_Representation_1](images/RNN_Recurrent_Neuron_Representation_1.jpg) | ![RNN_Recurrent_Neuron_Representation_2](images/RNN_Recurrent_Neuron_Representation_2.jpg) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unrolled RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [A Gentle Introduction to RNN Unrolling](https://machinelearningmastery.com/rnn-unrolling/)\n",
    "  - Recurrent neural networks are a type of neural network where the outputs from previous time steps are fed as input to the current time step\n",
    "  - RNNs are fit and make predictions over many time steps. We can simplify the model by unfolding or unrolling the RNN graph over the input sequence.\n",
    "    - ![Example-of-an-RNN-with-a-cycle](images/Example-of-an-RNN-with-a-cycle.png)*Example-of-an-RNN-with-a-cycle*\n",
    "  - Consider the case where we have multiple time steps of input (X(t), X(t+1), …), multiple time steps of internal state (u(t), u(t+1), …), and multiple time steps of outputs (y(t), y(t+1), …).\n",
    "  - We can unfold the above network schematic into a graph without any cycles.\n",
    "    - ![Example-of-Unrolled-RNN-on-the-forward-pass](images/Example-of-Unrolled-RNN-on-the-forward-pass.png)*Example-of-Unrolled-RNN-on-the-forward-pass*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [v1] Lecture 55\n",
    "  - We can unroll the RNN through time and we can share the values of the hidden weights through time and we can compute the output in the same maner that we had computed the output in ANN. Only change that we will see is the computation of $\\large h_t$ using previous value that you had stored in the memory $\\large h_{t-1}$\n",
    "  - $\\large h_0$ is the initial state, $\\large x_0$ is the initial input value\n",
    "  - Using $\\large h_0$ and $\\large x_0$, we compute the activation $\\large h_0$, using which we compute the output $\\large y_0$\n",
    "\n",
    "  - ![RNN_Unrolled_RNN](images/RNN_Unrolled_RNN.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Unrolled in Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Below diagram shows the different representation of RNN unrolled over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_Unrolled_In_Time](images/RNN_Unrolled_In_Time.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN-Based Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character based LM - RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used in keyboard word prediction where while typing the characters, the model has to predict the word and the upcoming words (when a word is typed)\n",
    "- In this language model, character will be the input\n",
    "  - Input will be a __*One-Hot-Vector*__\n",
    "  - Since character will be the input, the size of the vocabulary will be $26$, number of alphabets in English\n",
    "  - Example: For the word $\\texttt{success}$, characters $\\texttt{s}$, $\\texttt{u}$, $\\texttt{c}$, $\\texttt{c}$, $\\texttt{e}$, $\\texttt{s}$ are inputted one by one as a time series\n",
    "    - First $\\texttt{s}$ is given as input and it is expected to give $\\texttt{u}$ as output, if not the error is backpropagated\n",
    "    - Next $\\texttt{u}$ is given as input and it is expected to give $\\texttt{c}$ as output, if not the error is backpropagated\n",
    "    - Above steps are repeated until the stop symbol $\\texttt{\\$}$ is encounted.\n",
    "    - Multiple epochs will be executed until the model is trained.\n",
    "    - Once trained, the model can predict the next character after each current character input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_Character_Based_LM](images/RNN_Character_Based_LM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word based Language Model - RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similar to character based model, we can have Word based LM as well\n",
    "- Input will be a __*One-Hot-Vector*__\n",
    "- We will start inputting one word at a time until encoutering end of sentence symbol\n",
    "- Unlike traditional ANN, this is flexible, in the sense that, you can increase the time slice depending on the lenght of the sentence, and then you can make the system learn, which will give the model, which can be used for prediction\n",
    "  - Theoreticall, we can go for very very long sentence\n",
    "- Normally this model is used for generating text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Word LM | Example of Word LM |\n",
    "| :---: | :---: |\n",
    "| ![RNN_Word_Based_LM_1](images/RNN_Word_Based_LM_1.jpg) | ![RNN_Word_Based_LM_2](images/RNN_Word_Based_LM_2.jpg) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages and Trouble with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Theoretically, it is possible to store all historical information in the RNN\n",
    "  - Unlike traditional ANN, this is flexible, in the sense that, you can increase the time slice depending on the lenght of the sentence, and then you can make the system learn, which will give the model, which can be used for prediction\n",
    "  - Theoreticall, we can go for very very long sentence\n",
    "  - When we go for a long time series\n",
    "    - We get into the problem of __*Vanishing Gradient*__\n",
    "    - In some cases, we get into the problem of __*Exploding Gradient*__\n",
    "- Vanishing gradient problem - The diminishing value of $\\large \\delta$ makes it difficult to capture the long term memory as we move down the memory lane or layers of hidden nodes\n",
    "  - What is the soltuion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPTT-Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Chapter 7: The Simple Recurrent Network: A Simple Model that Captures the Structure in Sequences](https://web.stanford.edu/group/pdplab/pdphandbook/handbookch8.html)\n",
    "- [A Beginner's Guide to LSTMs and Recurrent Neural Networks](https://skymind.ai/wiki/lstm)\n",
    "- [How to implement a simple RNN](https://peterroelants.github.io/posts/rnn-implementation-part01/)\n",
    "- [Rolling and Unrolling RNNs](https://shapeofdata.wordpress.com/2016/04/27/rolling-and-unrolling-rnns/)\n",
    "- [Finding Structure in Time](https://crl.ucsd.edu/~elman/Papers/fsit.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
