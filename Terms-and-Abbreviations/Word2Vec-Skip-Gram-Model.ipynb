{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [v1] Lecture 44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Skip_Gram_Model_2](images/Skip_Gram_Model_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture - A Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Skip_Gram_Neural_Network_Architecture__A_Sample](images/Skip_Gram_Neural_Network_Architecture__A_Sample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation for Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lecturer gave only pieces of code, we need to integrate and make it compelte and run to see how it works\n",
    "- The corpuse used contains about 100 documents\n",
    "  - It is enough to identify all the word vectors in the right context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def setup_corpus(self, corpus_dir='/home/ramaseshan/Dropbox/NLPClass/2019/SmallCorpus/'):\n",
    "    self.corpus= PlaintextCorpusReader(corpus_dir, '.*')\n",
    "    \n",
    "def init_model_parameters(self, context_window_size=5, word_embedding_size=70 epochs=400, eta=0.01):\n",
    "    self.context_window_size = context_window_size\n",
    "    self.word_embedding_size = word_embedding_size\n",
    "    self.epochs = epochs\n",
    "    self.eta = eta\n",
    "    \n",
    "def initialize_weights(self):\n",
    "    self.embedding_weights = np.random.uniform(-0.9, 0.9, (self.vocabulary_size, self.word_embedding_size)) # input weights\n",
    "    self.context_weights = np.random.uniform(-0.9, 0.9, (self.word_embedding_size, self.vocabulary_size)) # input weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward Pass\n",
    "  - $\\displaystyle \\large H = W^TX$\n",
    "    - $H$ is the hidden layer\n",
    "  - $\\displaystyle \\large U = W^TH = W^{'T} \\cdot W^TX$\n",
    "    - $U$ is the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def forward_pass(self, X):\n",
    "    H = np.dot(self.embedding_weights.T, X)\n",
    "    U = np.dot(self.context_weights.T, H)\n",
    "    y_hat = self.softmax(U)\n",
    "    return y_hat, H, U\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Back Propagation\n",
    "  - $\\displaystyle \\large \\mathcal{w}_{ij}^{' new} = \\mathcal{W}_{ij}^{' old} - \\eta e_j \\cdot \\mathcal{h}_i$ or\n",
    "  - $\\displaystyle \\large \\bf{v}_{w_j}^{(new)} = \\bf{v}_{w_j}^{'(old)} - \\eta e_j \\cdot \\mathcal{h} \\text{for h = 1,2,3,...V}$\n",
    "  - $\\displaystyle \\large \\frac{\\partial{E}}{\\partial{w_{ki}}} = \\frac{\\partial{E}}{\\partial{h_{i}}} \\cdot \\frac{\\partial{h_i}}{\\partial{w_{ki}}} = EH_i \\cdot \\bf{\\chi}_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def back_propagation(self,X,H,E):\n",
    "  delta_context_weights = np.outer(H, E)\n",
    "  delta_embedding_weights = np.outer(X, np.dot(self.context_weights, E.T))\n",
    "  \n",
    "  # Change the weights using the back propagation values\n",
    "  self.context_weights = self.context_weights - (self.eta * delta_context_weights)\n",
    "  self.embedding_weights = self.embedding_weights - (self.eta * delta_embedding_weights)\n",
    "  pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training\n",
    "  - $\\displaystyle \\large E = -\\mathcal{v}_{wO}^{'} \\cdot \\mathcal{h} + log \\sum_{{j^{'}}=1}^{V} \\exp({v_{w_j}^{'}}^T \\cdot h)$\n",
    "- Training Guidelines\n",
    "  - The Error to Epoch curve should be smooth, otherwise there is something wrong in\n",
    "    - Learning Parameters\n",
    "  - Start with less number of EPOCH first to check the curve\n",
    "  - Don't start with huge size of vocabulary like 1 million, instead start with say 10 wordds, 5 epochs, kind of\n",
    "  - Make sure program is right\n",
    "  - Make sure that error is slowly coming down, then you can take a bigger corpus, bigger vocabulary and then increase the number of epochs, so on\n",
    "    - While doing this, keep changing the learning parameter, and find the right learning parameter as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train(self):\n",
    "    for i in range(0, self.epochs):\n",
    "        for target_word, context_words in np.array(self.training_samples):\n",
    "            # for all the words\n",
    "            y_hat, H, U = self.forward_pass(target_word)\n",
    "            \n",
    "            # Compute error for all the context words\n",
    "            EI = np.sum([np.subtract(y_hat, word) for word in context_words], axis=0)\n",
    "            \n",
    "            # do back propagation to adjust weights\n",
    "            self.back_propagation(target_word, H, EI)\n",
    "            \n",
    "            # Compute the error\n",
    "            self.error[i] = -np.sum([U[word.index(1)]\n",
    "                                   for word in context_words]) + \\\n",
    "                                        len(context_words) * \\\n",
    "                                        np.log(np.sum(np.exp(U)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vector for _deep_ and similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Python_Impl_Word_Embedding_5](images/Python_Impl_Word_Embedding_5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Preparation for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Python_Impl_Source_Preparation_For_Training](images/Python_Impl_Source_Preparation_For_Training.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity in Number of Neurons vs Number of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since input to Skip-Gram is a One-Hot vector and the output layer will have same number of neurons\n",
    "  - This huge size will impact in every layer\n",
    "    - Input to Hidden layer - say 1 million to 300\n",
    "    - Hidden to Output 300 to 1 millon\n",
    "    - 1 million softmax calculation\n",
    "- We need to reduce this complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
