{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [v1] Lecture 44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Skip_Gram_Model_2](images/Skip_Gram_Model_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture - A Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Skip_Gram_Neural_Network_Architecture__A_Sample](images/Skip_Gram_Neural_Network_Architecture__A_Sample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation for Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lecturer gave only pieces of code, we need to integrate and make it compelte and run to see how it works\n",
    "- The corpuse used contains about 100 documents\n",
    "  - It is enough to identify all the word vectors in the right context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def setup_corpus(self, corpus_dir='/home/ramaseshan/Dropbox/NLPClass/2019/SmallCorpus/'):\n",
    "    self.corpus= PlaintextCorpusReader(corpus_dir, '.*')\n",
    "    \n",
    "def init_model_parameters(self, context_window_size=5, word_embedding_size=70 epochs=400, eta=0.01):\n",
    "    self.context_window_size = context_window_size\n",
    "    self.word_embedding_size = word_embedding_size\n",
    "    self.epochs = epochs\n",
    "    self.eta = eta\n",
    "    \n",
    "def initialize_weights(self):\n",
    "    self.embedding_weights = np.random.uniform(-0.9, 0.9, (self.vocabulary_size, self.word_embedding_size)) # input weights\n",
    "    self.context_weights = np.random.uniform(-0.9, 0.9, (self.word_embedding_size, self.vocabulary_size)) # input weights\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward Pass\n",
    "  - $\\displaystyle \\large H = W^TX$\n",
    "    - $H$ is the hidden layer\n",
    "  - $\\displaystyle \\large U = W^TH = W^{'T} \\cdot W^TX$\n",
    "    - $U$ is the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def forward_pass(self, X):\n",
    "    H = np.dot(self.embedding_weights.T, X)\n",
    "    U = np.dot(self.context_weights.T, H)\n",
    "    y_hat = self.softmax(U)\n",
    "    return y_hat, H, U\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Back Propagation\n",
    "  - $\\displaystyle \\large \\mathcal{w}_{ij}^{' new} = \\mathcal{W}_{ij}^{' old} - \\eta e_j \\cdot \\mathcal{h}_i$ or\n",
    "  - $\\displaystyle \\large \\bf{v}_{w_j}^{(new)} = \\bf{v}_{w_j}^{'(old)} - \\eta e_j \\cdot \\mathcal{h} \\text{for h = 1,2,3,...V}$\n",
    "  - $\\displaystyle \\large \\frac{\\partial{E}}{\\partial{w_{ki}}} = \\frac{\\partial{E}}{\\partial{h_{i}}} \\cdot \\frac{\\partial{h_i}}{\\partial{w_{ki}}} = EH_i \\cdot \\bf{\\chi}_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def back_propagation(self,X,H,E):\n",
    "  delta_context_weights = np.outer(H, E)\n",
    "  delta_embedding_weights = np.outer(X, np.dot(self.context_weights, E.T))\n",
    "  \n",
    "  # Change the weights using the back propagation values\n",
    "  self.context_weights = self.context_weights - (self.eta * delta_context_weights)\n",
    "  self.embedding_weights = self.embedding_weights - (self.eta * delta_embedding_weights)\n",
    "  pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training\n",
    "  - $\\displaystyle \\large E = -\\mathcal{v}_{wO}^{'} \\cdot \\mathcal{h} + log \\sum_{{j^{'}}=1}^{V} \\exp({v_{w_j}^{'}}^T \\cdot h)$\n",
    "- Training Guidelines\n",
    "  - The Error to Epoch curve should be smooth, otherwise there is something wrong in\n",
    "    - Learning Parameters\n",
    "  - Start with less number of EPOCH first to check the curve\n",
    "  - Don't start with huge size of vocabulary like 1 million, instead start with say 10 wordds, 5 epochs, kind of\n",
    "  - Make sure program is right\n",
    "  - Make sure that error is slowly coming down, then you can take a bigger corpus, bigger vocabulary and then increase the number of epochs, so on\n",
    "    - While doing this, keep changing the learning parameter, and find the right learning parameter as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def train(self):\n",
    "    for i in range(0, self.epochs):\n",
    "        for target_word, context_words in np.array(self.training_samples):\n",
    "            # for all the words\n",
    "            y_hat, H, U = self.forward_pass(target_word)\n",
    "            \n",
    "            # Compute error for all the context words\n",
    "            EI = np.sum([np.subtract(y_hat, word) for word in context_words], axis=0)\n",
    "            \n",
    "            # do back propagation to adjust weights\n",
    "            self.back_propagation(target_word, H, EI)\n",
    "            \n",
    "            # Compute the error\n",
    "            self.error[i] = -np.sum([U[word.index(1)]\n",
    "                                   for word in context_words]) + \\\n",
    "                                        len(context_words) * \\\n",
    "                                        np.log(np.sum(np.exp(U)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vector for _deep_ and similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Python_Impl_Word_Embedding_5](images/Python_Impl_Word_Embedding_5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Preparation for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Python_Impl_Source_Preparation_For_Training](images/Python_Impl_Source_Preparation_For_Training.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity in Number of Neurons vs Number of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since input to Skip-Gram is a One-Hot vector and the output layer will have same number of neurons\n",
    "  - This huge size will impact in every layer\n",
    "    - Input to Hidden layer - say 1 million to 300\n",
    "    - Hidden to Output 300 to 1 millon\n",
    "    - 1 million softmax calculation\n",
    "- We need to reduce this complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Complexity (Optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need to start reducing the complexity from the training samples\n",
    "  - Some bi-grams won't add any value in learning context\n",
    "    - E.g., $\\texttt{(of, the)}$, $\\texttt{(returns, the)}$ won't contribute much in finding the relationship between the words $\\texttt{happy}$ and $\\texttt{returns}$\n",
    "  - Similarly in tri-grams it will exist\n",
    "  - Normally bigrams and tri-grams won't be used in the training, instead we will go with __*5-words window*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing following kind of words from the training samples, the resultant sub-samples are used for training\n",
    "  - Words pairs that does not give much informatino\n",
    "  - Words pairs that appear in switched order\n",
    "  - Less frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The words $\\texttt{(of, the)}$ in the pairs $\\texttt{(of, happy)}$, $\\texttt{(returns, the)}$ do __*not give much information*__ about the words happy and returns, respectively. Similarly some pairs __*reappear*__ with the __*order of the words switched*__\n",
    "  - The pair $\\texttt{(of, the)}$ does not add any value information\n",
    "  - The pairs $\\texttt{(wish, you)}$, $\\texttt{(you, wish)}$ can be reduced to one pair\n",
    "- Some word could also be __*randomly removed from the based on the frequencies*__\n",
    "  - Support a word appeared only once in the corpus, that won't give any information to the model\n",
    "    - This word can be removed from the training samples\n",
    "- Words with _less frequency or infrequent_ __*words appearing as context words*__ _could be discarded as they_ __*may not provide contextual information to the central word*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Sampling in Google's Word2Vec.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is the code for sub-sampling used by [word2vec.c](https://github.com/tmikolov/word2vec/blob/master/word2vec.c) that randomly removes a word from the sample\n",
    "\n",
    "```c\n",
    "        if (word == 0) break;\n",
    "        // The subsampling randomly discards frequent words while keeping the ranking same\n",
    "        if (sample > 0) {\n",
    "          real ran = (sqrt(vocab[word].cn / (sample * train_words)) + 1) * (sample * train_words) / vocab[word].cn;\n",
    "          next_random = next_random * (unsigned long long)25214903917 + 11;\n",
    "          if (ran < (next_random & 0xFFFF) / (real)65536) continue;\n",
    "        }\n",
    "```\n",
    "\n",
    "- Let $\\displaystyle \\large f(X) = \\frac{vocab[word].cn}{train \\_ words}$ and $\\displaystyle \\large ran = (\\sqrt{f(x)} + 1) \\times \\frac{1}{f(x)}$\n",
    "  - where\n",
    "    - ${vocab[word].cn}$ is the count of the word\n",
    "    - ${train \\_ words}$ represents all the training words\n",
    "  - Then, the probability of keeping the word is decided based on the generated random value random\n",
    "- If $ran \\lt random$ keep it, else discard the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __*Negative-Sampling:*__\n",
    "  - Another mechanism to minimize the computation\n",
    "  - The size of the network is proportional to the size of the vocabulary $\\mathbf{V}$. For every training cycle of input, the every weight in the network needs to be updated\n",
    "  - For every training cycle, Softmax function computes the sum of the output neuron values\n",
    "  - Cost of updating all the weights in the fully connected network is very high\n",
    "  - Is it possible to change only a small percentage of the weights?\n",
    "- __*Intuition behind negative sampling*__\n",
    "  - In case of Skip-Gram model, the learning happens between the context words and the input word\n",
    "    - The error that occurs doesn't have impact on other words that are in the training.\n",
    "    - Other than input and context words, rest of the words doesn't contribute anything to the learning in that iteration\n",
    "  - Can we really curtail the updation of the weights by only choosing the certain number of weights to be updated\n",
    "    - Why should I update all my weights in every iteration, when it does n't make any impact on the learning\n",
    "      - Computation of weights in each iteration is costly\n",
    "    - Which are the weights that we should not bother about\n",
    "- __*How Negative Sampling is performed*__\n",
    "  - Select a small number of _negative_ words\n",
    "    - Negative words are words that are not the context words\n",
    "    - __*These words are choosen by soem mechasnim*__\n",
    "  - While updating the weights, these samples output zero while the positive sample(s) will retainits value\n",
    "  - During the backpropagation, the weights related to the negative and positive words are changed and the rest will remain untouched for the current update\n",
    "  - This reduces drastically the computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting a Negative Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Several mechanism available in selecting negative words\n",
    "- Below is one kind of mechanism\n",
    "  1. Pick the most frequently occuring words as negative words\n",
    "    - Given the probability of a given word in the corpus $\\displaystyle \\large P(w_i) = \\frac{f(w_i)}{\\sum_{{j=0}}^{n} f(w_j)}$\n",
    "    - Apply some mechanism ($\\frac{3}{4}$), which boosts the probability of less frequent words and reduces the probability of highly appearing words in the corpus\n",
    "  2. Once we have the probability of words, find the number of occurence of that word in the entire corpus (say using Frequency Table)\n",
    "    - Using Frequency Table, find out how many times you want to repeat certain words in a table\n",
    "      - E.g., create a unigram table of size of the vocabulary\n",
    "      - Pickup the most frequently appearing word and create another unigram table\n",
    "      - Use the probability (above formula) to pick words which you want to use as negative sample words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word2Vec_Selectin_A_Negative_Sample](images/Word2Vec_Selectin_A_Negative_Sample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouble with the size of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All weights ($output \\rightarrow hidden$) and ($hidden \\rightarrow input$) are adjusted by taking a training sample so that the prediction cycle minimizes the loss function\n",
    "- This amounts to updating all the weights in the neural network\n",
    "  - amounts to several million weights for a network which has input neurons, $|V| = 1M$, and hidden unit size as 300\n",
    "- In addition, we should consider the seveeral million training samples pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As part of reducing the complexity of Word2Vec learning, optimization can be applied to Softmax as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word2Vec_Softmax](images/Word2Vec_Softmax.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
