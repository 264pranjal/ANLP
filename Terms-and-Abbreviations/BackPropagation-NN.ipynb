{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Refer [19]\n",
    "  - Instructor details follows Chapter 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Backpropagation_Steps.jpg] Backpropagation_Steps.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [22]\n",
    "  - first adjust last set of weights\n",
    "  - propagate error back to each previous layer\n",
    "  - adjust their weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal of _backpropagation_ is to change the weights so that the $\\text{estimated target} \\approx \\text{target}$, thereby minimizing the error for each neuron and the network as a whole\n",
    "  - $J(\\theta) = \\text{1/2} \\large((t_1 - y_1)^2 + (t_2 - y_2)^2)$\n",
    "    - $J(\\theta)$ is the _cost function_\n",
    "      - [Calculate the error](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/) for each output using the [squared error function](https://en.wikipedia.org/wiki/Backpropagation#Derivation) and sum them to get the total error\n",
    "      - The $\\text{1/2}$ is used to cancel the square in error function, so that we can take the derivative easily\n",
    "        - $E_{total} = \\sum \\frac{1}{2}(target - output)^2$\n",
    "    - We want to adjust weights coming in and going out of hidden layer so that $t - y$ is minimized\n",
    "    - $\\Delta W \\propto - \\cfrac{\\delta J(\\theta)}{\\delta W}$\n",
    "      - The rate of change of _cost_ will give you the change with which we have to update the weight\n",
    "    - Example \n",
    "      - $\\large w^5 = w^4 - \\eta \\frac{\\delta J(\\theta)}{\\delta w^4}$\n",
    "    - ![Backpropagation_Model](images/Backpropagation_Model.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivation of Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Derivation of Backpropagation](https://www.cs.swarthmore.edu/~meeden/cs81/s10/BackPropDeriv.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Backpropagation_Model_Forward_Pass.](images/Backpropagation_Model_Forward_Pass.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do we compute the change of values for $V$\n",
    "  - We know the error $J(\\theta)$\n",
    "  - We need to make changes so that the error $J(\\theta)$ becomes smaller\n",
    "  - It is done by backpropagating the error from the output to the output weights\n",
    "- It can be done simply using _partial differential equation_\n",
    "  - We will use [Chain Rule](https://en.wikipedia.org/wiki/Chain_rule) for identifying the weight changes\n",
    "  \n",
    "\n",
    "- ![Backward_Pass_Adjust_Hidden_Output_Layer_Weights_1](images/Backward_Pass_Adjust_Hidden_Output_Layer_Weights_1.jpg)\n",
    "- ![Backward_Pass_Adjust_Hidden_Output_Layer_Weights_2](images/Backward_Pass_Adjust_Hidden_Output_Layer_Weights_2.jpg)\n",
    "- ![Backward_Pass_Adjust_Hidden_Output_Layer_Weights_3](images/Backward_Pass_Adjust_Hidden_Output_Layer_Weights_3.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Initialize Weights\n",
    "1. Feed Input\n",
    "2. Compute $z, h, g, y$ - This is referred as _Forward Pass_\n",
    "3. Backpropagation\n",
    "  - 3.1 Compute $\\Delta V$ and $\\Delta W$\n",
    "4. Update $V^{new} = V^{old} - \\eta \\Delta V$\n",
    "5. Update $W^{new} = W^{old} - \\eta \\Delta W$\n",
    "\n",
    "- The training is repeated until either\n",
    "  - There is no more change in $V^{new}$ and $W^{new}$\n",
    "  - Or\n",
    "  - Until certain number of iterations\n",
    "  - Or\n",
    "  - $\\Delta J \\theta$ is very small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
