{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [v1] Lecture 51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN use cases in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using Word2Vec, we were able to understand the word and were able to capture the neighbourhood relationship of that word\n",
    "- Once the word is predicted, next we want to\n",
    "  - Understand the Sentence\n",
    "  - Understand the Paragraph\n",
    "  - Understand the Document\n",
    "  - Para Phrase/ Summarize\n",
    "  - Translate the paragraph from one language to the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Applications in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Keyboard prediction - What could be the next word\n",
    "  - NLP is used in predicting the next word\n",
    "2. Searching for a topic in a Search Engline (Eg., Google Search Engine) - Lots of options dropping down relating to the typed query\n",
    "  - NLP is behind it in predicting the most possible query matching to the words that you have typed\n",
    "3. Option to complete the sentece - E.g., GMail option to complete the sentence while typing the eMail\n",
    "  - Note that it provides the possible _sentence_, not the words like case 1\n",
    "  \n",
    "- In all of the applications, size of the __*input is not limited by the words*__\n",
    "  - You can have two characters typed, or 3 words typed or 5 words typed, and so on...\n",
    "  - Once you start typing character, it keeps providing the right word for you\n",
    "  - Once you complete the first word, it starts suggesting the next possible word for you\n",
    "  - Once you have second word, it starts suggesting complete sentece for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NLP_Applications_Problems'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Looking at the application, is it possible solve these problems with?\n",
    "  - Vector Space Model\n",
    "  - Probabilistic Language Model\n",
    "  - ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_NLP_Applications_In_Action](images/RNN_NLP_Applications_In_Action.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN for LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Study Link\n",
    "  - [An Introduction to Recurrent Neural Networks](https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912>) gives more details on the limitations of Traditional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using __*Traditional Neural Network*__ (Fixed Input Neural Networks)\n",
    "  - Is it possible to use the NN for solving the [NLP Applications problem](#NLP_Applications_Problems)?\n",
    "  - It is possible, However\n",
    "    - The window size is fixed (in case of Word2Vec)\n",
    "    - But the problems said above are limited by any size\n",
    "    - If we use ANN (Referrring Word2Vec kind of ANN, traditional NN), we need to change the model for each increase in number of words in those problems\n",
    "      - It is not easy to reuse (for longer sequence of words) the model what we have trained\n",
    "  - In Traditional Neural Networks, there is a restriction that input layer size is fixed\n",
    "  - Traditiona NN does not bother about the sequence of words. Meaning, time series (sequence data) cannot be used in Traditional NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_ANN_For_LM](images/RNN_ANN_For_LM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Fixed Input Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embeddings are learned based on a _small local window_ surronding words\n",
    "  - $\\textrm{good}$ and $\\textrm{bad}$ share the almost the same embedding\n",
    "    - E.g., In this sentence $\\textrm{good or bad take it}$\n",
    "    - In this $\\textrm{good}$ is appearing as contex for $\\textrm{bad}$, but both are opposite words\n",
    "      - Both are not similar, but embedding learnt it. It is not a problem of Neural Network.\n",
    "      - It is because of the problem of the way we have constructed the sentence\n",
    "- Does not address __*polysemy*__\n",
    "  - $\\textrm{The boys play cricket on the banks of a river}$\n",
    "  - $\\textrm{The boys play cricket near a national bank}$\n",
    "  - In above sentences, $\\textrm{bank}$ refers to different places - river side palce and actual financial transaction place\n",
    "  - Small window size is not enough to understand the meaning of the entire sentence. Hence traditional NN has limitations for this.\n",
    "- Does not use frequencies of _term co-occurences_\n",
    "  - For e.g., in CBOW and Skip-Gram models, frequency of the word is completely ingnored\n",
    "- Word embedding provide _distributed vectors for words_\n",
    "  - How about phrases? $\\textrm{India Today}$, $\\textrm{Indian Express}$, $\\textrm{The Sun News}$\n",
    "    - Word Embedding represnts only words as vectors, not phrases like the one listed above, which as adjoint\n",
    "    - Traiditional NN can't understand the phrases like above\n",
    "  - Can we encode a sentence as a distributed vector - _Sentence Vectors_?\n",
    "  - How about paragraphs? _Paragraph Vectors_?\n",
    "- Memory less and does not bother where the words and context came from\n",
    "- Not able to handle variable length text\n",
    "- Some NLP tasks require semantic modeling over the whole sentence\n",
    "  - Machine Translation\n",
    "  - Question answering, Chat-bots\n",
    "  - Text Summarization\n",
    "- The data is considered as static - does not depend on a sequence of time\n",
    "- They are location invariant\n",
    "- Some important tasks depend on the sequence of data\n",
    "  - $(y(t+1) = f(x(t),x(t-1),x(t-2),...,x(t-n))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Learning and its Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sequence Learning is the study of machine learning algorithms designed for applications that require sequential data or temporal data\n",
    "  - Example 1\n",
    "    - Translation of a speech in English to Chinese speech\n",
    "  - Example 2\n",
    "    - A professional is speaking in a conference. He might be referring to his talks that are in initial time of session in later portion of that session\n",
    "    - System should be able to understand the setences, understand the contexts, and when the reference happens, it should be able to understand in relation to that initial speech\n",
    "    - This requires sequential learning, as sequence of sentences (speech) need to understood in that order for more context information\n",
    "- We model the speech in the Time-Series in the NLP and use that to the NN as input, to make it understand the sequence, for the required task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applciations (Uses RNN over Time Series Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Named Entity Recognition (NER)\n",
    "  - Example: \"Mr.John is the CEO of the Company. And he had done great things for the company\"\n",
    "    - What does that \"he\" means there in the second sentece?\n",
    "    - Sytem should be able to say that \"he\" refers to the CEO of that company\n",
    "    - It is called NER.\n",
    "  - How many times does the 'CEO' referred in the documennt? Is it possible to find that?\n",
    "    - NER model should be used, which can recognize that person as CEO, wherever he is mentioned as part of the document.\n",
    "- Paraphrase detection - identifying semantically equivalent questions\n",
    "  - A question can be asked in different ways\n",
    "  - All those questions are semantically equivalent\n",
    "  - Paraphrase detection is finding semantically equivalent sentences\n",
    "  - Example: IT Call Center\n",
    "    - They receive calls having various queries, but semantically equivalent\n",
    "    - Company need to find most frequently asked questions (FAQ), so that new joinee can answer those calls\n",
    "    - Here paraphrase detection is required\n",
    "- Language Generation\n",
    "  - 'Given a photograph, you are asked to write a line about the photograph'\n",
    "    - You look at the content of the photograph, say objects and you give some title\n",
    "    - We can use Language Generation Model\n",
    "      - Input is going to be different\n",
    "        - Meaning, the photograph may have 3 object, or 5 object or any number of objects in it\n",
    "        - We should be able to process those without changing or adjusting our neural network size\n",
    "- Machine Translation\n",
    "  - Given a parallel corpora, we should be able to translate from one language to the other\n",
    "  - We have to do sentence by sentence translation to have correct translation (not word by word translation, which won't give correct translation)\n",
    "- Speech Recognition\n",
    "  - Should be able to translation a speech audio file from one language to the other\n",
    "  - Based on the speech, system should be able to recognize what he is speaking about\n",
    "    - Example: Wreck a nice beach or recognize speech\n",
    "      - Based on the context, it should be able to understand that the speaker said \"recognize speech\"\n",
    "    - This can be acheived in NLP, when we have taken words as time-series\n",
    "- Automatically generating subtitles for a video\n",
    "- Spell Checking\n",
    "  - When you type, you should really be able to figure out the distance between the characters that you have typed so-far and the words that are in the dictionary, start suggesting what is the right word.\n",
    "- Predictive Typing\n",
    "- Chat-bots/ Dialog Understanding\n",
    "  - An application which should understand the sentences and provide some input to the user\n",
    "    - Eg., Customer Service Chat bot. Based on user input, it should provide input to the user\n",
    "- Generate/ Correct Hard-written text\n",
    "  - OCR cannot be used always for generating text from hand written text\n",
    "    - E.g., - ![SL_Hard_Written_Text](images/SL_Hard_Written_Text.jpg)\n",
    "    - In this, it is unclear whether \"the quick __brown__ fo\" or \"the quick __frown__ fo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
