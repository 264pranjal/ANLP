{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding using Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is developed by Google (around 2013, 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Process each word in a Vocabulary of words to obtain a respective numeric representation of each word in Vocabulary\n",
    "  - Instead of have a _One Hot Vector_, represent words in terms fixed-sized vector having, 100 or 200 or 300 elements\n",
    "- Reflect _Sematic Similarities_, _Syntactic Similarities_, or both, between words they represent\n",
    "- Map each of the plurality of words to a respective vector and output a single merged vector that is a combination of the respective vectors\n",
    "  - Merge multiple words that are similar and put them in one vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Words and Central Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Probabilitis Language Model\n",
    "  - Conditional Probability is used in identifying/predicting the next word in the Language Model\n",
    "  - In Language Model, the word that is going to be predicted is the last word in the context of words\n",
    "  - So, when the context of words is given, the next word of context is predicted\n",
    "    - Example: $\\text{\"How are you\"}$\n",
    "      - $\\text{\"How are\"}$ are the given context words in the case of Language Model\n",
    "      - $\\text{\"you\"}$ is the context word that we want to predict\n",
    "- In CBOW Model\n",
    "  - Central Word is surrounded by context words\n",
    "  - Given the context words, we want to identify/predict what is my _Central Word_?\n",
    "    - Example: $\\text{\"more happy returns of the day\"}$, lets consider window size as $5$\n",
    "      - $\\text{\"more happy ___ of the day\"}$ are the given context words in the case of CBOW Model\n",
    "      - $\\text{\"returns\"}$ is the central word that we want to identify/predict\n",
    "- In Skip-Gram Model\n",
    "  - Given the central word, identify the surronding words\n",
    "  - Example: $\\text{\"more happy returns of the day\"}$, lets consider window size as $5$\n",
    "    - $\\text{\"returns\"}$ is the given context word\n",
    "    - $\\text{\"more happy of the\"}$ is what we need to predict the surronding context words for the given central word\n",
    "\n",
    "- ![Context_Words_and_Central_Word](images/Context_Words_and_Central_Word.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CBOW Neural Network Architecture\n",
    "  - Input layer having $n-1$ words, where $n$ is the window size\n",
    "    - For window size $n=5\"$, $w_{t-2},w_{t-1},w_{t+1},w_{t+2}$\n",
    "  - Neuron sum's (linear sum) all the incoming weights, input\n",
    "  - Finally we have output, which predicts the central word\n",
    "    - $w_{t}$\n",
    "    - $Softmax$ is used to find the most probable central word\n",
    "- Input is a __*One Hot Vector*__\n",
    "  - So we cannot feed all the words togehter as one shot in the input layer\n",
    "  - We will be feeding one word at a time\n",
    "     - Example: \"Wish\", \"you\", \"a\", \"happy\", \"year\" as context words\n",
    "       - Each words are inputed to the input layer one at a time\n",
    "- Perform a Linear Summation\n",
    "  - Over the input and its weights\n",
    "- Maximize the probability of word based on the word co-occurences within a distance $n$\n",
    "- Input size and Output size should match\n",
    "  - If the input vector size is 100, output vector size should also be 100\n",
    "  - $Softmax$ probabiltiy will be estiamted over those 100 words, indicating which is more probable as central word\n",
    "\n",
    "![CBOW_NN_Architecture](images/CBOW_NN_Architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Skip Gram Neural Network Architecture\n",
    "\n",
    "- It is similar to CBOW\n",
    "  - Input is a __*One Hot Vector*__\n",
    "  - Output predicts one word at a time based on the given central word\n",
    "- Example\n",
    "  - SG uses the central word \"new\" and predicts the context words \"WIsh\" \"you\", \"a\", \"happy\", \"year\"\n",
    "\n",
    "![Skip_Gram_Model_NN_Architecture.jpg](images/Skip_Gram_Model_NN_Architecture.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
