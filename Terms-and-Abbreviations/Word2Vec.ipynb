{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding using Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It is developed by Google (around 2013, 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [27]\n",
    "  - Good overview of all models, more details on\n",
    "    - Context Words\n",
    "    - CBOW\n",
    "    - Skip-Gram\n",
    "    - Negative Sampling\n",
    "- From [v4]\n",
    "  - More intuitive explanation on\n",
    "    - Limitations of One-Hot-Vector\n",
    "    - Idea behind the Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional Similarity based representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [v4] Lecture 2\n",
    "  - You can get a lot of value by representing a word by means of its neighbors\n",
    "    - \"You shall know a word by the company it keeps\" J.R.Firth 1957:11\n",
    "  - This is one of the most successful ideas of modern statistical NLP\n",
    "  - ![Distributional_Similarity_Based_Representations](images/Distributional_Similarity_Based_Representations.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Meanings is defined in terms of Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [v4] Lecture 2\n",
    "  - We build a dense vector for each word type, chosen so that it is good at predicting other words appearing in the context\n",
    "    - ... those words are also being represented by vectors ... it all gets a bit recursive\n",
    "    - Similarity of words are found using metrix like dot product of those vectors\n",
    "  - ![Word_Meaning_As_Vector](images/Word_Meaning_As_Vector.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Idea of Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [v4] Lecture 2\n",
    "  - __*Predict between every word and its context words!*__\n",
    "  - Two Alogirhtms\n",
    "    - Continuous Bag of Words (CBOW)\n",
    "    - Skip-Grams (SG)\n",
    "  - Two (moderately efficient) training methods\n",
    "    - Hierarchical softmax\n",
    "    - Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Process each word in a Vocabulary of words to obtain a respective numeric representation of each word in Vocabulary\n",
    "  - Instead of have a _One Hot Vector_, represent words in terms fixed-sized vector having, 100 or 200 or 300 elements\n",
    "- Reflect _Sematic Similarities_, _Syntactic Similarities_, or both, between words they represent\n",
    "- Map each of the plurality of words to a respective vector and output a single merged vector that is a combination of the respective vectors\n",
    "  - Merge multiple words that are similar and put them in one vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Context Words](https://cs224d.stanford.edu/lecture_notes/notes1.pdf) and Central Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Probabilitis Language Model\n",
    "  - Conditional Probability is used in identifying/predicting the next word in the Language Model\n",
    "  - In Language Model, the word that is going to be predicted is the last word in the context of words\n",
    "  - So, when the context of words is given, the next word of context is predicted\n",
    "    - Example: $\\text{\"How are you\"}$\n",
    "      - $\\text{\"How are\"}$ are the given context words in the case of Language Model\n",
    "      - $\\text{\"you\"}$ is the context word that we want to predict\n",
    "- In CBOW Model\n",
    "  - Central Word is surrounded by context words\n",
    "  - Given the context words, we want to identify/predict what is my _Central Word_?\n",
    "    - Example: $\\text{\"more happy returns of the day\"}$, lets consider window size as $5$\n",
    "      - $\\text{\"more happy ___ of the day\"}$ are the given context words in the case of CBOW Model\n",
    "      - $\\text{\"returns\"}$ is the central word that we want to identify/predict\n",
    "- In Skip-Gram Model\n",
    "  - Given the central word, identify the surronding words\n",
    "  - Example: $\\text{\"more happy returns of the day\"}$, lets consider window size as $5$\n",
    "    - $\\text{\"returns\"}$ is the given context word\n",
    "    - $\\text{\"more happy of the\"}$ is what we need to predict the surronding context words for the given central word\n",
    "\n",
    "- ![Context_Words_and_Central_Word](images/Context_Words_and_Central_Word.jpg)\n",
    "\n",
    "- From [27] Notes 1\n",
    "  - __Context of a Word__\n",
    "    - The context of a word is the set of $C$ surronding words.\n",
    "    - For instance, for $C = 2$, the context of the word $\\text{\"fox\"}$ in the sentence $\\text{\"The quick brown fox jumped over the lazy dog\"}$ is $\\{\\text{\"quick\"}, \\text{\"brown\"}, \\text{\"jumped\", \\text{\"over\"}}\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Refer [25] to get to know more about architecture of both models_\n",
    "\n",
    "- CBOW Neural Network Architecture\n",
    "  - Input layer having $n-1$ words, where $n$ is the window size\n",
    "    - For window size $n=5\"$, $w_{t-2},w_{t-1},w_{t+1},w_{t+2}$\n",
    "  - Neuron sum's (linear sum) all the incoming weights, input\n",
    "  - From [v4] Lecture 2, __*Window size $n$ is one of the Hyper Parameter for this model*__\n",
    "  - Finally we have output, which predicts the central word\n",
    "    - $w_{t}$\n",
    "    - $Softmax$ is used to find the most probable central word\n",
    "- Input is a __*One Hot Vector*__\n",
    "  - So we cannot feed all the words togehter as one shot in the input layer\n",
    "  - We will be feeding one word at a time\n",
    "     - Example: $\\text{\"Wish\"}$, $\\text{\"you\"}$, $\\text{\"a\"}$, $\\text{\"happy\"}$, $\\text{\"year\"}$ as context words\n",
    "       - Each words are inputed to the input layer one at a time\n",
    "- Perform a Linear Summation\n",
    "  - Over the input and its weights\n",
    "- Maximize the probability of word based on the word co-occurences within a distance $n$\n",
    "- Input size and Output size should match\n",
    "  - If the input vector size is 100, output vector size should also be 100\n",
    "  - $Softmax$ probabiltiy will be estiamted over those 100 words, indicating which is more probable as central word\n",
    "\n",
    "![CBOW_NN_Architecture](images/CBOW_NN_Architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [v4] Lecture 2\n",
    "  - Words as Vectors\n",
    "  - Window Size $n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Skip Gram Neural Network Architecture\n",
    "\n",
    "- It is similar to CBOW\n",
    "  - Input is a __*One Hot Vector*__\n",
    "  - Output predicts one word at a time based on the given central word\n",
    "- Example\n",
    "  - SG uses the central word $\\text{\"new\"}$\" and predicts the context words $\\text{\"Wish\"}$ $\\text{\"you\"}$, $\\text{\"a\"}$, $\\text{\"happy\"}$, $\\text{\"year\"}$\n",
    "- From [v4] Lecture 2, __*Window size $n$ is one of the Hyper Parameter for this model*__\n",
    "\n",
    "![Skip_Gram_Model_NN_Architecture.jpg](images/Skip_Gram_Model_NN_Architecture.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [v4] Lecture 2\n",
    "  - Words as Vectors\n",
    "  - Window Size $n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
