{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPTT - Exploding and Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From [v1] Lecture 59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN solved many problems that are possible with traditional neural networks\n",
    "- As people started using RNN for solving their problems, they faced two inherent issues in RNN:\n",
    "  - __*Exploding Gradient*__\n",
    "    - Values became very big, for which computer started generating 'NaN'\n",
    "  - __*Vanishing Gradient*__\n",
    "    - Values became very extermely small, very close to zero but not equal to zero\n",
    "- When above issue happens, no action takes place in Neural Network\n",
    "  - Neural Network unable to learn\n",
    "    - You need a good gradient, so that network moves towards equillibrium/ local minima which solves the problem\n",
    "    - If you don't have a good gradient, the training stops, so it can't learn\n",
    "- In order to design a good RNN network for various purposes, we __*should understand these two difficulties*__ that we would face __*in design of RNN*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Exploding/Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_Exploding_Vanishing_Gradient_1](images/RNN_Exploding_Vanishing_Gradient_1.jpg)\n",
    "![RNN_Exploding_Vanishing_Gradient_2](images/RNN_Exploding_Vanishing_Gradient_2.jpg)\n",
    "![RNN_Exploding_Vanishing_Gradient_3](images/RNN_Exploding_Vanishing_Gradient_3.jpg)\n",
    "![RNN_Exploding_Vanishing_Gradient_4](images/RNN_Exploding_Vanishing_Gradient_4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN is claimed to be that input sequence can be of any length\n",
    "- But from below example, it is clear that RNN network starts forgetting its memories (See [Gradient Clipping](#Gradient_Clipping) to know more)\n",
    "  - Due to Vanishing Gradient\n",
    "  - With the values (rough values given by lecturer, instead of proving it mathematically), we see that differtiation is very less\n",
    "- That is, RNN is not able to remember long sequence\n",
    "  - The value ($U$) vanishes when the word that you want to predict is found somewhere very far away in the same sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_Exploding_Vanishing_Gradient_Example](images/RNN_Exploding_Vanishing_Gradient_Example.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Gradient_Clipping'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The values of $\\large U, V$ and $W$ are unbounded. We do not have any boundary on those.\n",
    "  - If you look at the values of activation function for $\\large s_t$ and $\\large z_t$, we always have the value between $[-1 \\texttt{and} +1]$ and $[0 \\texttt{and} 1]$ respectively\n",
    "  - In case of $U$ and $V$, they are not bounded by this condition, i.e., they are not conditioned by values\n",
    "  - So it is possible for $U$ to become very small during backpropagation, and when it becomes extremely small, the long-term remembarance is going to be gone(vanished) or the long-term dependency is gone(vanished).\n",
    "    - It means, it only remembers the short-term\n",
    "- This implies that nothing that RNN forget, but the gradient that we are moving from the last time state to the first time state getting smaller and smaller, which causes this issue\n",
    "  - The values of $U$, $V$ and $W$ are not conditioned, because of that, gradient explodes or vanishes\n",
    "- The gradient is either very large or very small.\n",
    "  - This can cause the optimizer to converge slowly\n",
    "- To speed up training, clip the gradient at certain values\n",
    "  - If $\\large g \\lt 1$, or if $\\large g \\gt 1$, then $\\large g = 1$\n",
    "  - Or\n",
    "  - if $\\large ||g|| \\gt \\textit{threshold}$, then $\\large g \\leftarrow \\dfrac{threshold}{||g||}g$\n",
    "- Clip the gradient if it exceeds a $threshold$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
