{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reducing Complexity (Optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We need to start reducing the complexity from the training samples\n",
    "  - Some bi-grams won't add any value in learning context\n",
    "    - E.g., $\\texttt{(of, the)}$, $\\texttt{(returns, the)}$ won't contribute much in finding the relationship between the words $\\texttt{happy}$ and $\\texttt{returns}$\n",
    "  - Similarly in tri-grams it will exist\n",
    "  - Normally bigrams and tri-grams won't be used in the training, instead we will go with __*5-words window*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing following kind of words from the training samples, the resultant sub-samples are used for training\n",
    "  - Words pairs that does not give much informatino\n",
    "  - Words pairs that appear in switched order\n",
    "  - Less frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The words $\\texttt{(of, the)}$ in the pairs $\\texttt{(of, happy)}$, $\\texttt{(returns, the)}$ do __*not give much information*__ about the words happy and returns, respectively. Similarly some pairs __*reappear*__ with the __*order of the words switched*__\n",
    "  - The pair $\\texttt{(of, the)}$ does not add any value information\n",
    "  - The pairs $\\texttt{(wish, you)}$, $\\texttt{(you, wish)}$ can be reduced to one pair\n",
    "- Some word could also be __*randomly removed from the based on the frequencies*__\n",
    "  - Support a word appeared only once in the corpus, that won't give any information to the model\n",
    "    - This word can be removed from the training samples\n",
    "- Words with _less frequency or infrequent_ __*words appearing as context words*__ _could be discarded as they_ __*may not provide contextual information to the central word*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Sampling in Google's Word2Vec.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is the code for sub-sampling used by [word2vec.c](https://github.com/tmikolov/word2vec/blob/master/word2vec.c) that randomly removes a word from the sample\n",
    "\n",
    "```c\n",
    "        if (word == 0) break;\n",
    "        // The subsampling randomly discards frequent words while keeping the ranking same\n",
    "        if (sample > 0) {\n",
    "          real ran = (sqrt(vocab[word].cn / (sample * train_words)) + 1) * (sample * train_words) / vocab[word].cn;\n",
    "          next_random = next_random * (unsigned long long)25214903917 + 11;\n",
    "          if (ran < (next_random & 0xFFFF) / (real)65536) continue;\n",
    "        }\n",
    "```\n",
    "\n",
    "- Let $\\displaystyle \\large f(X) = \\frac{vocab[word].cn}{train \\_ words}$ and $\\displaystyle \\large ran = (\\sqrt{f(x)} + 1) \\times \\frac{1}{f(x)}$\n",
    "  - where\n",
    "    - ${vocab[word].cn}$ is the count of the word\n",
    "    - ${train \\_ words}$ represents all the training words\n",
    "  - Then, the probability of keeping the word is decided based on the generated random value random\n",
    "- If $ran \\lt random$ keep it, else discard the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __*Negative-Sampling:*__\n",
    "  - Another mechanism to minimize the computation\n",
    "  - The size of the network is proportional to the size of the vocabulary $\\mathbf{V}$. For every training cycle of input, the every weight in the network needs to be updated\n",
    "  - For every training cycle, Softmax function computes the sum of the output neuron values\n",
    "  - Cost of updating all the weights in the fully connected network is very high\n",
    "  - Is it possible to change only a small percentage of the weights?\n",
    "- __*Intuition behind negative sampling*__\n",
    "  - In case of Skip-Gram model, the learning happens between the context words and the input word\n",
    "    - The error that occurs doesn't have impact on other words that are in the training.\n",
    "    - Other than input and context words, rest of the words doesn't contribute anything to the learning in that iteration\n",
    "  - Can we really curtail the updation of the weights by only choosing the certain number of weights to be updated\n",
    "    - Why should I update all my weights in every iteration, when it does n't make any impact on the learning\n",
    "      - Computation of weights in each iteration is costly\n",
    "    - Which are the weights that we should not bother about\n",
    "- __*How Negative Sampling is performed*__\n",
    "  - Select a small number of _negative_ words\n",
    "    - Negative words are words that are not the context words\n",
    "    - __*These words are choosen by soem mechasnim*__\n",
    "  - While updating the weights, these samples output zero while the positive sample(s) will retainits value\n",
    "  - During the backpropagation, the weights related to the negative and positive words are changed and the rest will remain untouched for the current update\n",
    "  - This reduces drastically the computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Negative Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Several mechanism available in selecting negative words\n",
    "- Below is one kind of mechanism\n",
    "  1. Pick the most frequently occuring words as negative words\n",
    "    - Given the probability of a given word in the corpus $\\displaystyle \\large P(w_i) = \\frac{f(w_i)}{\\sum_{{j=0}}^{n} f(w_j)}$\n",
    "    - Apply some mechanism ($\\frac{3}{4}$), which boosts the probability of less frequent words and reduces the probability of highly appearing words in the corpus\n",
    "  2. Once we have the probability of words, find the number of occurence of that word in the entire corpus (say using Frequency Table)\n",
    "    - Using Frequency Table, find out how many times you want to repeat certain words in a table\n",
    "      - E.g., create a unigram table of size of the vocabulary\n",
    "      - Pickup the most frequently appearing word and create another unigram table\n",
    "      - Use the probability (above formula) to pick words which you want to use as negative sample words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word2Vec_Selectin_A_Negative_Sample](images/Word2Vec_Selectin_A_Negative_Sample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trouble with the size of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All weights ($output \\rightarrow hidden$) and ($hidden \\rightarrow input$) are adjusted by taking a training sample so that the prediction cycle minimizes the loss function\n",
    "- This amounts to updating all the weights in the neural network\n",
    "  - amounts to several million weights for a network which has input neurons, $|V| = 1M$, and hidden unit size as 300\n",
    "- In addition, we should consider the seveeral million training samples pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As part of reducing the complexity of Word2Vec learning, optimization can be applied to Softmax as well\n",
    "- Softmax complexity is $\\mathcal{O}(|V|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word2Vec_Softmax](images/Word2Vec_Softmax.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Softmax (H-Softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decompose the flat hierarchy into a binary tree\n",
    "- Form a hierarchical description of a word as a sequence of $\\mathcal{O}(log_2 |V|)$ decision and there by reducing the computing complexity of Softmax\n",
    "  - $\\mathcal{O}(|V|) \\rightarrow \\mathcal{O}(log_2(|V|))$\n",
    "- Lay the words in a tree-based hierarchy - words as leaves\n",
    "- Binary tree with $|V| - 1$ nodes for $left (0)$ and $right (1)$ traversal\n",
    "- Every leave represents the probability of the word\n",
    "- Path length of a Balanced Tree is $log_2(|V|)$.\n",
    "  - If the $|V| = \\texttt{1 million words}$, then the $length = \\texttt{19.9 bits/word}$\n",
    "- Constructing an _Huffman encoded-tree_ would help frequent words to have short unique binary codes\n",
    "- Learn to take these probabilistic decisions instead of directly predicting each word's probability $\\textbf{[Bengio:2003:NPL:944919.944966]}$\n",
    "- Every intermediate node denotes the relative probabilities of its child nodes\n",
    "- The path to reach every leaf (word) is unique\n",
    "- H-Softmax in many cases increases the prediction speed by more than $50X$ times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Tree Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![H_Softmax_Hierarchical_Tree_Structure](images/H_Softmax_Hierarchical_Tree_Structure.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decomposes the flat hierarchy into a binary tree\n",
    "- The path to reach every leaf (word) is unique\n",
    "- Lays the words in a tree-based hierarchy - words as leaves\n",
    "- Binary tree with $|V| - 1$ nodes for left and right traversal\n",
    "- Every intermediate node denotes the relative probabilities of its child nodes\n",
    "- Every leaf represents the normalized probability of the word\n",
    "- Each node is indexed by a bit vector corresponding to the path from the root to the node\n",
    "  - Append $1$ or $0$ according to whether the $left$ or $right$ branch of a decision node\n",
    "- Normalized values for the words are calcualted without finding the probability for every word\n",
    "- The entire vocabulary is partitioned into classes\n",
    "- ANN learns to take these probabilistic decisions instead of directly predicting each word's probability [[33]](References.ipynb)\n",
    "- Forms a hiearchical description of a word as a sequence of $\\mathcal{O}(log_2 |V|)$ decisions\n",
    "- Reduces the computing complexity of Softmax - $\\mathcal{O}(V) \\rightarrow \\mathcal{O}(log_2(|V|))$\n",
    "- Path length of a Balanced Tree is $log_2(|V|)$.\n",
    "  - If the $|V| = \\texttt{1 million words}$, then the $length = \\texttt{19.9 bits/word}$\n",
    "- A Balanced Binary Tree should provide an exponential speed-up, on the order of $\\frac{|V|}{log_2(|V|)}$\n",
    "- Constructing an __Huffman encoded-tree__ would help frequent words to have short unique binary codes\n",
    "- H-Softmax in many cases increases the prediction speed by more than $50X$ times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separate training is required for phrases\n",
    "- Embeddings are learned based n a small local window surrounding words - good and bad share the almost the same embedding\n",
    "- Does not address polysemy\n",
    "- Does not use frequencies of term co-occurences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Binary Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  Softmax_Optimization_Balanced_Binary_Tree_1 | Softmax_Optimization_Balanced_Binary_Tree_2 |\n",
    "| --- | --- |\n",
    "| ![Softmax_Optimization_Balanced_Binary_Tree_1](images/Softmax_Optimization_Balanced_Binary_Tree_1.jpg) | ![Softmax_Optimization_Balanced_Binary_Tree_2](images/Softmax_Optimization_Balanced_Binary_Tree_2.jpg) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Output Layer to Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The output of H-Softmax changed the structure of the output\n",
    "  - It doesn't match with the Neural Network's Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word2vec_WIth_Hierarchical_Softmax](images/Word2vec_WIth_Hierarchical_Softmax.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Weights using Hierarchical Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the core of recent NLP trends\n",
    "  - Read the [papers](#Optimization_Readings) suggested by lecturer to understand more on the new teminologies used in this section\n",
    "  - Watch videos and read papers multiple times to understand the concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| H_Softmax_Updating_Weights_1 | H_Softmax_Updating_Weights_2 | H_Softmax_Updating_Weights_3 |\n",
    "|----|----|---|\n",
    "| ![H_Softmax_Updating_Weights_1](images/H_Softmax_Updating_Weights_1.jpg) | ![H_Softmax_Updating_Weights_2](images/H_Softmax_Updating_Weights_2.jpg) | ![H_Softmax_Updating_Weights_3](images/H_Softmax_Updating_Weights_3.jpg) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the code given in the [link](https://github.com/dav/word2vec) to understand how Word2Vec is implemented in C, it is a Google Open Source\n",
    "- You can download, compile (make) and can use the scripts to run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word2Vec_Results](images/Word2Vec_Results.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2Vec is from Google\n",
    "- GloVE is from Stanford University\n",
    "- Both available in various word vector sizes - 50, 100, 30, ...\n",
    "  - Readymade Word Vectors available for public use\n",
    "- We may use the Word Embeddings(Word Vectors) from either of these (if in case we do not want to create our own Word Embeddings from our Corpus, which is a tedious process normally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Optimization_Readings'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Papers used by Lecturer for preparing the session (Word2Vec in ANLP course). Read it for further understanding about Word2Vec\n",
    "  1. [Yoshua Bengio et all. \"A Neural Probabilitis Language Model\"](https://dl.acm.org/citation.cfm?id=94419.944966)\n",
    "    - __*Must Read*__, A Very Important Paper, available publicly\n",
    "  2. [Tomas Mikolov et al. \"Distributed Representations of Words and Phrases and Their Compositionality\"](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "    - <https://dl.acm.org/citation.cfm?id=2999959>\n",
    "    - __*Must Read*__, it talks about Word2Vec\n",
    "  3. [Andrily Mnih and Geoffrey Hinton. \"A Scalable Hierarchical Distributed Language Model\"](https://dl.acm.org/citation.cfm?id=2981780.2981915)\n",
    "    - Talks about the\n",
    "      - Hierarchical Distributed Model\n",
    "      - How we can place the words in the binary trees in an hierarchical fashion\n",
    "  4. Jeffrey A. Dean Tomas Mikolov Kai ChenGregory S. Crrado.\n",
    "    - _U.S. pat. US9037464B1. May 2015_\n",
    "    - This is the original patent filed by Google\n",
    "    - Gives information about how these processes completed in finding the word vectors\n",
    "  5. Lecture 7105 from [NPTEL on Deep Learning](https://nptel.ac.in/courses/106/106/106106184/)\n",
    "    - Especially on the Hierarchical Softmax\n",
    "      - Briefly explains the theory of the Hierarchical Model, a good reference in understanding the material (material of this course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Word2Vec_Study_References](images/Word2Vec_Study_References.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
