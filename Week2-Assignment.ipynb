{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Implement the LSI using SVD. Use the the documents given below as\n",
    "samples.\n",
    "\n",
    "__Note__: Use Numpy's linalg package to understand the internals of SVD\n",
    "1. information extraction systems\n",
    "2. natural language processing\n",
    "3. speech signal systems\n",
    "4. speech processing\n",
    "\n",
    "Use \"speech systems\" as the query to find out whether the results match\n",
    "the answer for Question 5 (Assignment 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, similarities\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read (construct) the Corpus\n",
    "corpus = [\n",
    "    \"information extraction systems\",\n",
    "    \"natural language processing\",\n",
    "    \"speech signal systems\",\n",
    "    \"speech processing\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['information', 'extraction', 'systems'],\n",
       " ['natural', 'language', 'processing'],\n",
       " ['speech', 'signal', 'systems'],\n",
       " ['speech', 'processing']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize\n",
    "corpus_text = [doc.split() for doc in corpus]\n",
    "corpus_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://radimrehurek.com/gensim/tut2.html\n",
    "# Create Coprus Dictionary\n",
    "dictionary = corpora.Dictionary(corpus_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Document-Term Matrix (Bag-of-Words)\n",
    "doc_term_matrix = [dictionary.doc2bow(tokens) for tokens in corpus_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(3, 1), (4, 1), (5, 1)],\n",
       " [(2, 1), (6, 1), (7, 1)],\n",
       " [(5, 1), (7, 1)]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF Matrix - by fit(ting) the model\n",
    "tfidf = models.TfidfModel(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform (Apply) the model\n",
    "corpus_tfidf = tfidf[doc_term_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSI Transformation\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.606*\"speech\" + 0.523*\"processing\" + 0.398*\"signal\" + 0.256*\"systems\" + 0.233*\"language\" + 0.233*\"natural\" + 0.114*\"information\" + 0.114*\"extraction\"'),\n",
       " (1,\n",
       "  '-0.434*\"information\" + -0.434*\"extraction\" + -0.379*\"systems\" + 0.368*\"language\" + 0.368*\"natural\" + -0.323*\"signal\" + 0.321*\"processing\" + -0.025*\"speech\"'),\n",
       " (2,\n",
       "  '0.472*\"information\" + 0.472*\"extraction\" + 0.400*\"natural\" + 0.400*\"language\" + -0.351*\"signal\" + -0.324*\"speech\" + 0.060*\"systems\" + 0.051*\"processing\"'),\n",
       " (3,\n",
       "  '-0.600*\"signal\" + 0.437*\"processing\" + -0.351*\"language\" + -0.351*\"natural\" + 0.313*\"speech\" + -0.214*\"systems\" + 0.172*\"information\" + 0.172*\"extraction\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.606*\"speech\" + 0.523*\"processing\" + 0.398*\"signal\" + 0.256*\"systems\" + 0.233*\"language\" + 0.233*\"natural\" + 0.114*\"information\" + 0.114*\"extraction\"'),\n",
       " (1,\n",
       "  '-0.434*\"information\" + -0.434*\"extraction\" + -0.379*\"systems\" + 0.368*\"language\" + 0.368*\"natural\" + -0.323*\"signal\" + 0.321*\"processing\" + -0.025*\"speech\"'),\n",
       " (2,\n",
       "  '0.472*\"information\" + 0.472*\"extraction\" + 0.400*\"natural\" + 0.400*\"language\" + -0.351*\"signal\" + -0.324*\"speech\" + 0.060*\"systems\" + 0.051*\"processing\"'),\n",
       " (3,\n",
       "  '-0.600*\"signal\" + 0.437*\"processing\" + -0.351*\"language\" + -0.351*\"natural\" + 0.313*\"speech\" + -0.214*\"systems\" + 0.172*\"information\" + 0.172*\"extraction\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Query Document\n",
    "query_doc = \"speech systems\"\n",
    "query_vec_bow = dictionary.doc2bow(query_doc.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.8622500379254149),\n",
       " (1, -0.4031576893878383),\n",
       " (2, -0.2639245139336671),\n",
       " (3, 0.09848073003635041)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the query to LSI space\n",
    "query_vec_lsi = lsi[query_vec_bow]\n",
    "query_vec_lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform corpus to LSI space and index it\n",
    "# Reference: https://radimrehurek.com/gensim/tut3.html\n",
    "index = similarities.MatrixSimilarity(lsi[corpus_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain similarity against all the documents in the corpus\n",
    "sims = index[query_vec_lsi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.33579946), (1, 0.0), (2, 0.82253736), (3, 0.7123382)]\n"
     ]
    }
   ],
   "source": [
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "Try to improve the incomplete code (shown during the demo 2)(Topic\n",
    "modeling using BBC corpus) from the github\n",
    "\n",
    "<https://github.com/Ramaseshanr/anlp/blob/master/TopicModeling_LSI.ipynb>\n",
    "\n",
    "Once completed, post your experience in the discussion forum for the benefit of others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntujs/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import gensim\n",
    "from gensim.models import LsiModel\n",
    "from gensim import models\n",
    "from gensim import corpora\n",
    "from gensim.utils import lemmatize\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import EnglishStemmer # Yuva\n",
    "from nltk.stem import WordNetLemmatizer # Yuva\n",
    "from gensim.parsing.preprocessing import remove_stopwords, stem_text\n",
    "from gensim.parsing.preprocessing import strip_numeric, strip_short,strip_multiple_whitespaces,strip_non_alphanum,strip_punctuation,strip_tags,preprocess_string\n",
    "import pandas as pd\n",
    "from gensim import similarities\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data\n",
    "corpus_file_name = pathlib.Path(\"bbc-text.csv\")\n",
    "\n",
    "if corpus_file_name.exists():\n",
    "    corpus_dir = corpus_file_name\n",
    "else:\n",
    "    corpus_dir = 'https://raw.githubusercontent.com/Ramaseshanr/anlp/master/corpus/bbc-text.csv'\n",
    "    \n",
    "df_corpus = pd.read_csv(corpus_dir,names=['category', 'text'])\n",
    "\n",
    "if not corpus_file_name.exists():\n",
    "    df_corpus.to_csv(corpus_file_name,index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df_corpus['text'].values.tolist()\n",
    "corpus = corpus[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filter = [\n",
    "    lambda x: x.lower(), strip_tags, strip_punctuation,\n",
    "    strip_multiple_whitespaces, strip_numeric,\n",
    "    remove_stopwords, strip_short, stem_text\n",
    "]\n",
    "\n",
    "def preprocessing(corpus):\n",
    "    stemmer = EnglishStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words('english')\n",
    "    for document in corpus:\n",
    "        doc = strip_numeric(document)\n",
    "        doc = strip_multiple_whitespaces(doc)\n",
    "        doc = remove_stopwords(doc)\n",
    "        strip_tags(doc)\n",
    "        doc = strip_short(doc,3)\n",
    "        #doc = stem_text(doc)\n",
    "        doc = strip_punctuation(doc)\n",
    "        tokens = gensim.utils.tokenize(doc, lower=True)\n",
    "        tokens = [token for token in tokens if token not in stop_words ]\n",
    "        #tokens = [stemmer.stem(token) for token in tokens]\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        yield tokens\n",
    "        #yield gensim.utils.tokenize(doc, lower=True)\n",
    "\n",
    "def preprocessing2(corpus):\n",
    "    for document in corpus:\n",
    "        doc = strip_numeric(document)\n",
    "        doc = strip_multiple_whitespaces(doc)\n",
    "        doc = remove_stopwords(doc)\n",
    "        strip_tags(doc)\n",
    "        doc = strip_short(doc,3)\n",
    "        #doc = stem_text(doc)\n",
    "        doc = strip_punctuation(doc)\n",
    "        yield gensim.utils.tokenize(doc, lower=True)\n",
    "\n",
    "#texts = preprocessing(corpus)\n",
    "#for text in texts:\n",
    "#    print(list(text))\n",
    "\n",
    "texts = preprocessing(corpus)\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in tokens_:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save_as_text(\"corpus_dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_vocab = dictionary.itervalues()\n",
    "print(\"Corpus Vocab Size: \", len(corpus_vocab), \", Corpus Dictionary Size: \", len(dictionary.token2id))\n",
    "\n",
    "with open('corpus_vocab.txt', 'w') as f:\n",
    "    v = sorted(corpus_vocab, key=len)\n",
    "    for w in v:\n",
    "        f.write(w + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5c31ab28fa41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_extremes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_below\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'After filtering: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitervalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_as_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"corpus_dict2.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(no_below=1, keep_n=12000)\n",
    "print('After filtering: ', len(dictionary.itervalues()))\n",
    "dictionary.save_as_text(\"corpus_dict2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(tokens) for tokens in preprocessing(corpus)]\n",
    "tfidf = models.TfidfModel(doc_term_matrix)\n",
    "corpus_tfidf = tfidf[doc_term_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary)  # initialize an LSI transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi.projection.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi.projection.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lsi.print_topics(num_topics=5, num_words=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
